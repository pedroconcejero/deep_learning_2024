{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedroconcejero/deep_learning_2024/blob/main/neural_machine_translation_with_keras_hub_20epochs_revisado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZJa0_h7nUiY"
      },
      "source": [
        "# English-to-Spanish translation with KerasHub\n",
        "\n",
        "**Author:** [Abheesht Sharma](https://github.com/abheesht17/)<br>\n",
        "**Date created:** 2022/05/26<br>\n",
        "**Last modified:** 2024/04/30<br>\n",
        "**Revisado (Pedro):** 2024/12/02<br>\n",
        "**Description:** Use KerasHub to train a sequence-to-sequence Transformer model on the machine translation task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU7E97kInUic"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "KerasHub provides building blocks for NLP (model layers, tokenizers, metrics, etc.) and\n",
        "makes it convenient to construct NLP pipelines.\n",
        "\n",
        "In this example, we'll use KerasHub layers to build an encoder-decoder Transformer\n",
        "model, and train it on the English-to-Spanish machine translation task.\n",
        "\n",
        "This example is based on the\n",
        "[English-to-Spanish NMT\n",
        "example](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/)\n",
        "by [fchollet](https://twitter.com/fchollet). The original example is more low-level\n",
        "and implements layers from scratch, whereas this example uses KerasHub to show\n",
        "some more advanced approaches, such as subword tokenization and using metrics\n",
        "to compute the quality of generated translations.\n",
        "\n",
        "You'll learn how to:\n",
        "\n",
        "- Tokenize text using `keras_hub.tokenizers.WordPieceTokenizer`.\n",
        "- Implement a sequence-to-sequence Transformer model using KerasHub's\n",
        "`keras_hub.layers.TransformerEncoder`, `keras_hub.layers.TransformerDecoder` and\n",
        "`keras_hub.layers.TokenAndPositionEmbedding` layers, and train it.\n",
        "- Use `keras_hub.samplers` to generate translations of unseen input sentences\n",
        " using the top-p decoding strategy!\n",
        "\n",
        "Don't worry if you aren't familiar with KerasHub. This tutorial will start with\n",
        "the basics. Let's dive right in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s-a7oE1nUie"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Before we start implementing the pipeline, let's import all the libraries we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvGONBUZnUig",
        "outputId": "5b29ef6b-5fc4-4e9d-afe8-a45ebb39fb1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.1/644.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade rouge-score\n",
        "!pip install -q --upgrade keras-hub\n",
        "!pip install -q --upgrade keras  # Upgrade to Keras 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Ohunu2-nUij"
      },
      "outputs": [],
      "source": [
        "import keras_hub\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "import keras\n",
        "from keras import ops\n",
        "\n",
        "import tensorflow.data as tf_data\n",
        "from tensorflow_text.tools.wordpiece_vocab import (\n",
        "    bert_vocab_from_dataset as bert_vocab,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-egX3SunnUil"
      },
      "source": [
        "Let's also define our parameters/hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4aATteT8nUio"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20  # This should be at least 10 for convergence\n",
        "MAX_SEQUENCE_LENGTH = 40\n",
        "ENG_VOCAB_SIZE = 15000\n",
        "SPA_VOCAB_SIZE = 15000\n",
        "\n",
        "EMBED_DIM = 256\n",
        "INTERMEDIATE_DIM = 2048\n",
        "NUM_HEADS = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1qhcFf4nUiq"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "We'll be working with an English-to-Spanish translation dataset\n",
        "provided by [Anki](https://www.manythings.org/anki/). Let's download it:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL del archivo en GitHub\n",
        "url = \"https://raw.githubusercontent.com/pedroconcejero/deep_learning_2024/refs/heads/main/spa.txt\"\n",
        "\n",
        "# Descargar el archivo\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()  # Lanza una excepción si ocurre un error\n",
        "\n",
        "# Procesar las líneas directamente\n",
        "lines = response.text.strip().split(\"\\n\")\n",
        "pairs = [line.split(\"\\t\") for line in lines]\n",
        "\n",
        "# Mostrar algunos pares\n",
        "for eng, spa in pairs[:5]:\n",
        "    print(f\"Inglés: {eng} -> Español: {spa}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXPbsSNHudM5",
        "outputId": "781ddd91-fc65-4934-9d46-bf43092e8941"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inglés: Go. -> Español: Ve.\n",
            "Inglés: Go. -> Español: Vete.\n",
            "Inglés: Go. -> Español: Vaya.\n",
            "Inglés: Go. -> Español: Váyase.\n",
            "Inglés: Hi. -> Español: Hola.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mbVtH6Hmtfby"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UmiYeSbnUiu"
      },
      "source": [
        "## Parsing the data\n",
        "\n",
        "Each line contains an English sentence and its corresponding Spanish sentence.\n",
        "The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n",
        "Before adding the text to a list, we convert it to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2qYVMmCcnUiw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Now open the file\n",
        "#with open(text_file) as f:\n",
        "#    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    # Check if the line has a tab character\n",
        "    if \"\\t\" in line:\n",
        "        eng, spa = line.split(\"\\t\")\n",
        "        eng = eng.lower()\n",
        "        spa = spa.lower()\n",
        "        text_pairs.append((eng, spa))\n",
        "    else:\n",
        "        # Handle lines without a tab character (e.g., skip them or print a warning)\n",
        "        print(f\"Warning: Skipping line without tab character: {line}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-040GSnnUiy"
      },
      "source": [
        "Here's what our sentence pairs look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP5aW2ownUiz",
        "outputId": "427f3587-f02c-4cae-da2a-b4cffd9b1575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('the truth hurts.', 'la verdad duele.')\n",
            "('she is a very kind girl.', 'ella es una chica muy gentil.')\n",
            "(\"you're not guilty.\", 'no sos culpable.')\n",
            "('it is difficult to solve this problem.', 'es difícil resolver este problema.')\n",
            "('are you busy tomorrow afternoon?', '¿estás ocupado mañana por la tarde?')\n"
          ]
        }
      ],
      "source": [
        "# Check if text_pairs is empty and provide a message\n",
        "if not text_pairs:\n",
        "    print(\"Error: 'text_pairs' list is empty. Check the input file or data processing.\")\n",
        "else:\n",
        "    for _ in range(5):\n",
        "        print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP64A3cdnUi1"
      },
      "source": [
        "Now, let's split the sentence pairs into a training set, a validation set,\n",
        "and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idCsnIaZnUi3",
        "outputId": "cb7407b7-1f14-4219-d2ff-5c9891d7ac3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964 total pairs\n",
            "83276 training pairs\n",
            "17844 validation pairs\n",
            "17844 test pairs\n"
          ]
        }
      ],
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63lgx7eLnUi5"
      },
      "source": [
        "## Tokenizing the data\n",
        "\n",
        "We'll define two tokenizers - one for the source language (English), and the other\n",
        "for the target language (Spanish). We'll be using\n",
        "`keras_hub.tokenizers.WordPieceTokenizer` to tokenize the text.\n",
        "`keras_hub.tokenizers.WordPieceTokenizer` takes a WordPiece vocabulary\n",
        "and has functions for tokenizing the text, and detokenizing sequences of tokens.\n",
        "\n",
        "Before we define the two tokenizers, we first need to train them on the dataset\n",
        "we have. The WordPiece tokenization algorithm is a subword tokenization algorithm;\n",
        "training it on a corpus gives us a vocabulary of subwords. A subword tokenizer\n",
        "is a compromise between word tokenizers (word tokenizers need very large\n",
        "vocabularies for good coverage of input words), and character tokenizers\n",
        "(characters don't really encode meaning like words do). Luckily, KerasHub\n",
        "makes it very simple to train WordPiece on a corpus with the\n",
        "`keras_hub.tokenizers.compute_word_piece_vocabulary` utility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "S5MOiTscnUi6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
        "    word_piece_ds = tf_data.Dataset.from_tensor_slices(text_samples)\n",
        "    vocab = keras_hub.tokenizers.compute_word_piece_vocabulary(\n",
        "        word_piece_ds.batch(1000).prefetch(2),\n",
        "        vocabulary_size=vocab_size,\n",
        "        reserved_tokens=reserved_tokens,\n",
        "    )\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G18MpxTknUi7"
      },
      "source": [
        "Every vocabulary has a few special, reserved tokens. We have four such tokens:\n",
        "\n",
        "- `\"[PAD]\"` - Padding token. Padding tokens are appended to the input sequence\n",
        "length when the input sequence length is shorter than the maximum sequence length.\n",
        "- `\"[UNK]\"` - Unknown token.\n",
        "- `\"[START]\"` - Token that marks the start of the input sequence.\n",
        "- `\"[END]\"` - Token that marks the end of the input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UDO_VZgOnUi8"
      },
      "outputs": [],
      "source": [
        "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
        "eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)\n",
        "\n",
        "spa_samples = [text_pair[1] for text_pair in train_pairs]\n",
        "spa_vocab = train_word_piece(spa_samples, SPA_VOCAB_SIZE, reserved_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeKs-3scnUi-"
      },
      "source": [
        "Let's see some tokens!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Eu7fYx8nUi_",
        "outputId": "0109980a-9916-481a-d46e-4fad28930892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Tokens:  ['there', 'they', 'go', 'her', 'has', 'will', 're', 'time', 'how', 'll']\n",
            "Spanish Tokens:  ['le', 'ella', 'te', 'para', 'mary', 'las', 'más', 'al', 'yo', 'tu']\n"
          ]
        }
      ],
      "source": [
        "print(\"English Tokens: \", eng_vocab[100:110])\n",
        "print(\"Spanish Tokens: \", spa_vocab[100:110])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq5Jk011nUjA"
      },
      "source": [
        "Now, let's define the tokenizers. We will configure the tokenizers with the\n",
        "the vocabularies trained above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "V7TzbKRJnUjB"
      },
      "outputs": [],
      "source": [
        "eng_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=eng_vocab, lowercase=False\n",
        ")\n",
        "spa_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=spa_vocab, lowercase=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBL4V2E5nUjB"
      },
      "source": [
        "Let's try and tokenize a sample from our dataset! To verify whether the text has\n",
        "been tokenized correctly, we can also detokenize the list of tokens back to the\n",
        "original text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQdlKfy5nUjC",
        "outputId": "23804c49-79da-4121-f2bd-567885e2c11d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sentence:  my work is my passion.\n",
            "Tokens:  tf.Tensor([ 78 171  67  78 851 524  11], shape=(7,), dtype=int32)\n",
            "Recovered text after detokenizing:  my work is my passion .\n",
            "\n",
            "Spanish sentence:  mi trabajo es mi pasión.\n",
            "Tokens:  tf.Tensor([  97  160   85   97   44  669 1457   14], shape=(8,), dtype=int32)\n",
            "Recovered text after detokenizing:  mi trabajo es mi pasión .\n"
          ]
        }
      ],
      "source": [
        "eng_input_ex = text_pairs[0][0]\n",
        "eng_tokens_ex = eng_tokenizer.tokenize(eng_input_ex)\n",
        "print(\"English sentence: \", eng_input_ex)\n",
        "print(\"Tokens: \", eng_tokens_ex)\n",
        "print(\n",
        "    \"Recovered text after detokenizing: \",\n",
        "    eng_tokenizer.detokenize(eng_tokens_ex),\n",
        ")\n",
        "\n",
        "print()\n",
        "\n",
        "spa_input_ex = text_pairs[0][1]\n",
        "spa_tokens_ex = spa_tokenizer.tokenize(spa_input_ex)\n",
        "print(\"Spanish sentence: \", spa_input_ex)\n",
        "print(\"Tokens: \", spa_tokens_ex)\n",
        "print(\n",
        "    \"Recovered text after detokenizing: \",\n",
        "    spa_tokenizer.detokenize(spa_tokens_ex),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo4Zh4JLnUjD"
      },
      "source": [
        "## Format datasets\n",
        "\n",
        "Next, we'll format our datasets.\n",
        "\n",
        "At each training step, the model will seek to predict target words N+1 (and beyond)\n",
        "using the source sentence and the target words 0 to N.\n",
        "\n",
        "As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n",
        "\n",
        "- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n",
        "`encoder_inputs` is the tokenized source sentence and `decoder_inputs` is the target\n",
        "sentence \"so far\",\n",
        "that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target\n",
        "sentence.\n",
        "- `target` is the target sentence offset by one step:\n",
        "it provides the next words in the target sentence -- what the model will try to predict.\n",
        "\n",
        "We will add special tokens, `\"[START]\"` and `\"[END]\"`, to the input Spanish\n",
        "sentence after tokenizing the text. We will also pad the input to a fixed length.\n",
        "This can be easily done using `keras_hub.layers.StartEndPacker`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YIzrf77snUjD"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_batch(eng, spa):\n",
        "    batch_size = ops.shape(spa)[0]\n",
        "\n",
        "    eng = eng_tokenizer(eng)\n",
        "    spa = spa_tokenizer(spa)\n",
        "\n",
        "    # Pad `eng` to `MAX_SEQUENCE_LENGTH`.\n",
        "    eng_start_end_packer = keras_hub.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "        pad_value=eng_tokenizer.token_to_id(\"[PAD]\"),\n",
        "    )\n",
        "    eng = eng_start_end_packer(eng)\n",
        "\n",
        "    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `spa` and pad it as well.\n",
        "    spa_start_end_packer = keras_hub.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
        "        start_value=spa_tokenizer.token_to_id(\"[START]\"),\n",
        "        end_value=spa_tokenizer.token_to_id(\"[END]\"),\n",
        "        pad_value=spa_tokenizer.token_to_id(\"[PAD]\"),\n",
        "    )\n",
        "    spa = spa_start_end_packer(spa)\n",
        "\n",
        "    return (\n",
        "        {\n",
        "            \"encoder_inputs\": eng,\n",
        "            \"decoder_inputs\": spa[:, :-1],\n",
        "        },\n",
        "        spa[:, 1:],\n",
        "    )\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXCC_Xb-nUjE"
      },
      "source": [
        "Let's take a quick look at the sequence shapes\n",
        "(we have batches of 64 pairs, and all sequences are 40 steps long):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3Nt58RynUjF",
        "outputId": "2ecd0cdc-6420-4afe-838f-9e3308f6546b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 40)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 40)\n",
            "targets.shape: (64, 40)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7OuW6OznUjG"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Now, let's move on to the exciting part - defining our model!\n",
        "We first need an embedding layer, i.e., a vector for every token in our input sequence.\n",
        "This embedding layer can be initialised randomly. We also need a positional\n",
        "embedding layer which encodes the word order in the sequence. The convention is\n",
        "to add these two embeddings. KerasHub has a `keras_hub.layers.TokenAndPositionEmbedding `\n",
        "layer which does all of the above steps for us.\n",
        "\n",
        "Our sequence-to-sequence Transformer consists of a `keras_hub.layers.TransformerEncoder`\n",
        "layer and a `keras_hub.layers.TransformerDecoder` layer chained together.\n",
        "\n",
        "The source sequence will be passed to `keras_hub.layers.TransformerEncoder`, which\n",
        "will produce a new representation of it. This new representation will then be passed\n",
        "to the `keras_hub.layers.TransformerDecoder`, together with the target sequence\n",
        "so far (target words 0 to N). The `keras_hub.layers.TransformerDecoder` will\n",
        "then seek to predict the next words in the target sequence (N+1 and beyond).\n",
        "\n",
        "A key detail that makes this possible is causal masking.\n",
        "The `keras_hub.layers.TransformerDecoder` sees the entire sequence at once, and\n",
        "thus we must make sure that it only uses information from target tokens 0 to N\n",
        "when predicting token N+1 (otherwise, it could use information from the future,\n",
        "which would result in a model that cannot be used at inference time). Causal masking\n",
        "is enabled by default in `keras_hub.layers.TransformerDecoder`.\n",
        "\n",
        "We also need to mask the padding tokens (`\"[PAD]\"`). For this, we can set the\n",
        "`mask_zero` argument of the `keras_hub.layers.TokenAndPositionEmbedding` layer\n",
        "to True. This will then be propagated to all subsequent layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gMiuwn4GnUjG"
      },
      "outputs": [],
      "source": [
        "# Encoder\n",
        "encoder_inputs = keras.Input(shape=(None,), name=\"encoder_inputs\")\n",
        "\n",
        "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=ENG_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        ")(encoder_inputs)\n",
        "\n",
        "encoder_outputs = keras_hub.layers.TransformerEncoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(inputs=x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = keras.Input(shape=(None,), name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
        "\n",
        "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=SPA_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        ")(decoder_inputs)\n",
        "\n",
        "x = keras_hub.layers.TransformerDecoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "decoder_outputs = keras.layers.Dense(SPA_VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "decoder = keras.Model(\n",
        "    [\n",
        "        decoder_inputs,\n",
        "        encoded_seq_inputs,\n",
        "    ],\n",
        "    decoder_outputs,\n",
        ")\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    decoder_outputs,\n",
        "    name=\"transformer\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_73Rj0RynUjI"
      },
      "source": [
        "## Training our model\n",
        "\n",
        "We'll use accuracy as a quick way to monitor training progress on the validation data.\n",
        "Note that machine translation typically uses BLEU scores as well as other metrics,\n",
        "rather than accuracy. However, in order to use metrics like ROUGE, BLEU, etc. we\n",
        "will have decode the probabilities and generate the text. Text generation is\n",
        "computationally expensive, and performing this during training is not recommended.\n",
        "\n",
        "Here we only train for 1 epoch, but to get the model to actually converge\n",
        "you should train for at least 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D8vuJ0VunUjJ",
        "outputId": "cfb06e4b-b4b5-4afd-a39c-2c28d88b62a6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"transformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_and_position_embed… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m3,850,240\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbeddi…\u001b[0m │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m1,315,072\u001b[0m │ token_and_position_em… │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional_1 (\u001b[38;5;33mFunctional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15000\u001b[0m)    │      \u001b[38;5;34m9,283,992\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                           │                        │                │ transformer_encoder[\u001b[38;5;34m0\u001b[0m… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_and_position_embed… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,850,240</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbeddi…</span> │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,072</span> │ token_and_position_em… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,283,992</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                           │                        │                │ transformer_encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,449,304\u001b[0m (55.12 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,449,304</span> (55.12 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,449,304\u001b[0m (55.12 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,449,304</span> (55.12 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 78ms/step - accuracy: 0.8189 - loss: 1.4812 - val_accuracy: 0.8676 - val_loss: 0.8069\n",
            "Epoch 2/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.8717 - loss: 0.7773 - val_accuracy: 0.8913 - val_loss: 0.6254\n",
            "Epoch 3/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.8928 - loss: 0.6192 - val_accuracy: 0.9036 - val_loss: 0.5347\n",
            "Epoch 4/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 66ms/step - accuracy: 0.9027 - loss: 0.5428 - val_accuracy: 0.9084 - val_loss: 0.5055\n",
            "Epoch 5/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 65ms/step - accuracy: 0.9095 - loss: 0.4961 - val_accuracy: 0.9123 - val_loss: 0.4802\n",
            "Epoch 6/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.9145 - loss: 0.4634 - val_accuracy: 0.9146 - val_loss: 0.4709\n",
            "Epoch 7/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 66ms/step - accuracy: 0.9184 - loss: 0.4381 - val_accuracy: 0.9164 - val_loss: 0.4623\n",
            "Epoch 8/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.9220 - loss: 0.4169 - val_accuracy: 0.9177 - val_loss: 0.4617\n",
            "Epoch 9/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.9251 - loss: 0.3978 - val_accuracy: 0.9181 - val_loss: 0.4584\n",
            "Epoch 10/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 66ms/step - accuracy: 0.9279 - loss: 0.3815 - val_accuracy: 0.9197 - val_loss: 0.4568\n",
            "Epoch 11/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 66ms/step - accuracy: 0.9303 - loss: 0.3656 - val_accuracy: 0.9200 - val_loss: 0.4585\n",
            "Epoch 12/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 67ms/step - accuracy: 0.9324 - loss: 0.3527 - val_accuracy: 0.9211 - val_loss: 0.4586\n",
            "Epoch 13/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.9345 - loss: 0.3402 - val_accuracy: 0.9206 - val_loss: 0.4606\n",
            "Epoch 14/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.9366 - loss: 0.3282 - val_accuracy: 0.9207 - val_loss: 0.4624\n",
            "Epoch 15/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 67ms/step - accuracy: 0.9382 - loss: 0.3177 - val_accuracy: 0.9216 - val_loss: 0.4655\n",
            "Epoch 16/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.9397 - loss: 0.3082 - val_accuracy: 0.9213 - val_loss: 0.4659\n",
            "Epoch 17/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.9413 - loss: 0.2989 - val_accuracy: 0.9217 - val_loss: 0.4664\n",
            "Epoch 18/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 66ms/step - accuracy: 0.9428 - loss: 0.2904 - val_accuracy: 0.9219 - val_loss: 0.4722\n",
            "Epoch 19/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 66ms/step - accuracy: 0.9444 - loss: 0.2818 - val_accuracy: 0.9227 - val_loss: 0.4755\n",
            "Epoch 20/20\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 66ms/step - accuracy: 0.9454 - loss: 0.2754 - val_accuracy: 0.9226 - val_loss: 0.4729\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7eea694f7340>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YapPMRTTnUjK"
      },
      "source": [
        "## Decoding test sentences (qualitative analysis)\n",
        "\n",
        "Finally, let's demonstrate how to translate brand new English sentences.\n",
        "We simply feed into the model the tokenized English sentence\n",
        "as well as the target token `\"[START]\"`. The model outputs probabilities of the\n",
        "next token. We then we repeatedly generated the next token conditioned on the\n",
        "tokens generated so far, until we hit the token `\"[END]\"`.\n",
        "\n",
        "For decoding, we will use the `keras_hub.samplers` module from\n",
        "KerasHub. Greedy Decoding is a text decoding method which outputs the most\n",
        "likely next token at each time step, i.e., the token with the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6hrXge7nUjK",
        "outputId": "a248a27c-9f51-4e35-9d15-be5c88c67527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Example 0 **\n",
            "you can't get a job if you don't have useful skills.\n",
            "no podés conseguir un trabajo si no esqube esqubelá que esqula .\n",
            "\n",
            "** Example 1 **\n",
            "i don't feel like doing anything today.\n",
            "hoy no tengo ganas de hacer nada como hoy .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Error!\n",
        "# A partir de Gemini:\n",
        "# The variable encoder_input_tokens is likely already an EagerTensor,\n",
        "#created by the eng_tokenizer function. Therefore,\n",
        "# calling to_tensor() on it again is causing the error.\n",
        "\n",
        "def decode_sequences(input_sentences):\n",
        "    batch_size = 1\n",
        "\n",
        "    # Tokenize the encoder input.\n",
        "    encoder_input_tokens = ops.convert_to_tensor(eng_tokenizer(input_sentences))\n",
        "    if len(encoder_input_tokens[0]) < MAX_SEQUENCE_LENGTH:\n",
        "        pads = ops.full((1, MAX_SEQUENCE_LENGTH - len(encoder_input_tokens[0])), 0)\n",
        "        # Remove the redundant .to_tensor() call\n",
        "        encoder_input_tokens = ops.concatenate(\n",
        "            [encoder_input_tokens, pads], 1 # Changed from encoder_input_tokens.to_tensor() to encoder_input_tokens\n",
        "        )\n",
        "\n",
        "    # Define a function that outputs the next token's probability given the\n",
        "    # input sequence.\n",
        "    def next(prompt, cache, index):\n",
        "        logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n",
        "        # Ignore hidden states for now; only needed for contrastive search.\n",
        "        hidden_states = None\n",
        "        return logits, hidden_states, cache\n",
        "\n",
        "    # Build a prompt of length 40 with a start token and padding tokens.\n",
        "    length = 40\n",
        "    start = ops.full((batch_size, 1), spa_tokenizer.token_to_id(\"[START]\"))\n",
        "    pad = ops.full((batch_size, length - 1), spa_tokenizer.token_to_id(\"[PAD]\"))\n",
        "    prompt = ops.concatenate((start, pad), axis=-1)\n",
        "\n",
        "    generated_tokens = keras_hub.samplers.GreedySampler()(\n",
        "        next,\n",
        "        prompt,\n",
        "        stop_token_ids=[spa_tokenizer.token_to_id(\"[END]\")],\n",
        "        index=1,  # Start sampling after start token.\n",
        "    )\n",
        "    generated_sentences = spa_tokenizer.detokenize(generated_tokens)\n",
        "\n",
        "    # Check if generated_sentences is a list and extract the first element\n",
        "    if isinstance(generated_sentences, list):\n",
        "        generated_sentences = generated_sentences[0]\n",
        "\n",
        "    return generated_sentences\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for i in range(2):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequences([input_sentence])\n",
        "\n",
        "    # The translated variable now holds a string (or bytes)\n",
        "    # and can be decoded directly.\n",
        "#    translated = translated.decode(\"utf-8\")\n",
        "\n",
        "    translated = (\n",
        "        translated.replace(\"[PAD]\", \"\")\n",
        "        .replace(\"[START]\", \"\")\n",
        "        .replace(\"[END]\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "    print(f\"** Example {i} **\")\n",
        "    print(input_sentence)\n",
        "    print(translated)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09oG33zAnUjL"
      },
      "source": [
        "## Evaluating our model (quantitative analysis)\n",
        "\n",
        "There are many metrics which are used for text generation tasks. Here, to\n",
        "evaluate translations generated by our model, let's compute the ROUGE-1 and\n",
        "ROUGE-2 scores. Essentially, ROUGE-N is a score based on the number of common\n",
        "n-grams between the reference text and the generated text. ROUGE-1 and ROUGE-2\n",
        "use the number of common unigrams and bigrams, respectively.\n",
        "\n",
        "We will calculate the score over 30 test samples (since decoding is an\n",
        "expensive process)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghoBIWm0nUjM",
        "outputId": "9054d2e0-a867-4c22-c30f-842d54ed49a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('i accept your challenge.', 'acepto tu desafío.') acepto tu cafío .\n",
            "('a lot of people in our neighborhood own guns.', 'mucha gente en nuestro vecindario posee armas.') mucha gente está en nuestra propiarir armas .\n",
            "(\"i'm thinking about changing my supplier.\", 'estoy pensando en cambiar de proveedor.') estoy pensando en la superplaza .\n",
            "('i have been to london twice.', 'he estado en londres dos veces.') he estado en londres veces .\n",
            "('her look says that she loves you.', 'su mirada te dice que te ama.') su mirada dice que ella te ama .\n",
            "('a study found that almost 10% of men were obese in 2008. that was up from about 5% in 1980.', 'un estudio descubrió que casi un 10% de los hombres eran obesos en el 2008. esto es un aumento de casi un 5% en el 1980.') un manejo casi 10 % de 200 cuando los cor pedudutr en 19 % en 19 % de 20999998\n",
            "(\"i'll have to think about it.\", 'tendré que pensármelo.') me tendré que pensarlo .\n",
            "('thank you for not smoking.', 'gracias por no fumar.') gracias por no fumas .\n",
            "(\"they won't find it.\", 'no lo encontrarán.') no lo encontrarán .\n",
            "(\"he's not a good person.\", 'no es una buena persona.') no es una buena persona .\n",
            "(\"i've got to learn french.\", 'tengo que aprender francés.') tengo que aprender francés .\n",
            "('his condition could have been worse.', 'su condición podría haber sido peor.') su condición podría haber sido peor peor .\n",
            "('mary knows how to fight.', 'mary sabe pelear.') mary sabe pelear .\n",
            "('would you rather have a cup of coffee or a cup of tea?', '¿preferiríais tomaros una taza de café o una de té?') ¿ preferirías tomar una taza de café o tomar un vaso de té ?\n",
            "('did my father really die?', '¿en serio mi padre murió?') ¿ realmente mi padre murió ?\n",
            "('it sounds great!', '¡suena genial!') ¡ suena muy bien !\n",
            "(\"i've caught a bad cold.\", 'cogí un resfriado terrible.') me atrapé los comptas frío .\n",
            "('he is doing penance.', 'está haciendo penitencia.') está haciendo la lapresación .\n",
            "('the workers united to solve the problem.', 'los trabajadores se aliaron para solucionar el problema.') los trabajadores tienen para resolver el problema .\n",
            "(\"i didn't know you could cook this well, tom.\", 'no sabía que sabías cocinar tan bien, tom.') no sabía que podrías cocinarlo bien , tom .\n",
            "(\"i don't want to talk about it.\", 'no quiero hablar de ello.') no quiero hablar de ello .\n",
            "('i just need to know what tom wants.', 'solo necesito saber qué quiere tom.') solo necesito saber lo que quiere tom .\n",
            "('he knows how to play the piano.', 'él sabe como tocar el piano.') él sabe tocar el piano .\n",
            "('i went to the bakery.', 'fui a la panadería.') fui aleder la pana .\n",
            "('i wish my dream would come true.', 'deseo que mi sueño se hiciera realidad.') ojalá mi sueño se hiciera verdad .\n",
            "(\"tom doesn't actually live within boston city limits.\", 'tom realmente no vive dentro de los límites de la ciudad de boston.') en realidad tom no vive en boston .\n",
            "('i just need a second.', 'solo necesito un segundo.') solo necesito un segundo .\n",
            "(\"we've got something for you.\", 'tenemos algo para ti.') tenemos algo para ti .\n",
            "('it is wrong to tell a lie.', 'es un error decir mentiras.') está mal de decir una mentira .\n",
            "('buy two pizzas and get a free soft drink.', 'al comprar dos pizzas, gratis un refresco.') compra dos cotan la pizza y che plaza .\n",
            "('he broke the world record.', 'él rompió el récord mundial.') él rompió el mundo de grabaje .\n",
            "('he began to run.', 'él empezó a correr.') él empezó a correr .\n",
            "('when i was young, i tried to read as many books as i could.', 'cuando yo era joven intentaba leer tantos libros como pudiera.') cuando tenía joven , intenté de leer como había leído años .\n",
            "('she has been dyeing her hair black for years.', 'ella se ha estado tiñendo el pelo de negro durante años.') ella ha estado teñando su pelo de negro .\n",
            "(\"you're patient.\", 'eres paciente.') eres paciente .\n",
            "('i could kill you.', 'yo las podría matar.') yo te podría matar .\n",
            "('i was about to go out when the telephone rang.', 'estaba a punto de salir cuando sonó el teléfono.') estaba a punto de salir cuando sonó el teléfono .\n",
            "('we are all born mad.', 'todos nacemos locos.') todos nosotros nacistenos .\n",
            "(\"tom threw mary's book into the fire.\", 'tom tiró el libro de mary al fuego.') tom le arrojó a mary al libro del fuego .\n",
            "('he drinks too much.', 'él bebe demasiado.') él bebe demasiado .\n",
            "('i have been expecting you.', 'te he estado esperando.') te he estado esperando .\n",
            "(\"i don't know what i want to do.\", 'no sé qué quiero hacer.') no sé lo que quiero hacer .\n",
            "('i only have one mouth, but i have two ears.', 'solo tengo una boca, pero tengo dos oídos.') solo tengo la boca , pero tiene las dos orejas .\n",
            "(\"he didn't tell me everything.\", 'él no me lo contó todo.') él no me lo dijo todo .\n",
            "('i want to work here.', 'quiero trabajar acá.') quiero trabajar aquí .\n",
            "('find tom.', 'encuéntrelo a tomás.') encuentren a tom .\n",
            "(\"maybe i'm losing my mind.\", 'puede que me esté volviendo loco.') tal vez estoy perdiendo la cabeza .\n",
            "('women talk nonstop.', 'las mujeres hablan sin parar.') las mujeres no se a las mujeresplazan .\n",
            "('tom locked his sister in the closet.', 'tom encerró a su hermana en el armario.') tom cerró su hermana ingresó por el armario .\n",
            "('i have nothing to do with that.', 'no tengo nada que ver con eso.') no tengo nada que ver con eso .\n",
            "('you are my father.', 'tú eres mi padre.') eres mi padre .\n",
            "('he got me a watch.', 'él me dio un reloj.') él me recibió un reloj .\n",
            "('who do you want to talk to?', '¿con quién quiere usted hablar?') ¿ a quién quieres hablar ?\n",
            "(\"i'm not going to need to do that.\", 'no voy a necesitar hacer eso.') no voy a hacer eso .\n",
            "('tom was forced to make a radical decision.', 'tom se vio forzado a tomar una decisión radical.') tom fue forzó a tomar una decisión gavasa decisión .\n",
            "(\"i know you're upset about your car being totaled, but you weren't injured and you should be thankful to be alive.\", 'sé que estás molesta porque tu carro quedó destrozado, pero saliste ilesa y deberías estar feliz que estás viva.') sé que estás molesto por tus mejores colo .\n",
            "('while we were on holiday, a neighbor took care of our cat.', 'un vecino cuidó a nuestro gato mientras estábamos de vacaciones.') cuando fuimos camenas de vacaciones , le tomó nuestro vecino a nuestro vecino .\n",
            "('you must come in.', 'debes entrar.') debes entrar .\n",
            "(\"didn't tom predict that?\", '¿no lo había predicho tom?') ¿ no previsa tom ?\n",
            "('these garments are made of 100 percent wool.', 'esta ropa esta hecha cien por ciento de lana.') estos gartén son a los tartén 100 sintieron en un lotúl .\n",
            "('when was the last time you cried?', '¿cuándo fue la última vez que lloraste?') ¿ cuándo fue la última vez que llorabas lloró ?\n",
            "('tom did worse this year.', 'tom lo hizo peor este año.') tom hizo peor año .\n",
            "('do you know how to start a fire using just sticks of wood?', '¿sabes prender fuego usando solo palos de madera?') ¿ sabes encender fuego usando solo palo de madera ?\n",
            "('spain is the host country for the olympics in 1992.', 'españa fue la sede de las olimpiadas de 1992.') españa es el campo por el campo a las aripaxio a los 192 .\n",
            "(\"tom didn't turn off the lights.\", 'tom no apagó las luces.') tom no apareció la luz del almá .\n",
            "('you might meet tom if you go to the library.', 'puede que te encuentres con tom si vas a la biblioteca.') podrías que te encuentres si tom fuera a la biblioteca .\n",
            "(\"what's your favorite food?\", '¿cuál es su comida favorita?') ¿ cuál es tu comida favorita ?\n",
            "('when was the last time you chewed tobacco?', '¿cuándo fue la última vez que masticaste tabaco?') ¿ cuándo fue la última vez que atembás que cabás ?\n",
            "('she is a pianist.', 'ella es pianista.') ella es un pianista .\n",
            "('the gate was too narrow for the truck.', 'el portón era demasiado angosto para el camión.') el portón fue demasiado escitada para el camión .\n",
            "(\"don't blame them for what happened back in boston.\", 'no les culpes de lo que sucedió en boston.') no los colvisas por lo que pasó en boston .\n",
            "(\"i think it's going to take time.\", 'pienso que llevará tiempo.') creo que va a tiempo .\n",
            "('can you give an exact report of what happened?', '¿puede usted describir exactamente lo que pasó?') ¿ podés dar un exacto de lo que pasó ?\n",
            "('what do your friends call you?', '¿cómo les dicen sus amigos?') ¿ cómo te llames a tus amigos ?\n",
            "('we like games.', 'nos gustan los juegos.') nos gustan los juegos .\n",
            "('do you speak french?', '¿usted habla francés?') ¿ hablas francés ?\n",
            "(\"i'm better.\", 'yo soy mejor.') soy mejor .\n",
            "('the odor of dirty socks makes me gag.', 'el olor de calcetines sucios me da náusea.') los ortrudieron de los calcetines sucios gas .\n",
            "(\"i don't know whether i can do it, but i'll try.\", 'no sé si lo pueda hacer, pero lo voy a intentar.') no sé si puedo hacerlo , pero lo intentingé .\n",
            "('he is a very good teacher.', 'él es un muy buen profesor.') él es un muy buen profesor .\n",
            "('i no longer need your help.', 'ya no necesito tu ayuda.') ya no necesito tu ayuda .\n",
            "('what person does everyone take off his hat to?', '¿ante qué persona se quita todo el mundo el sombrero?') ¿ qué persona a todos se tardan de su sombrero ?\n",
            "('tom overslept.', 'tom se quedó dormido.') tom se durmió .\n",
            "('my father is expecting you to phone him tomorrow.', 'mi padre espera que le llames mañana.') mi padre te está esperando para mañana .\n",
            "('monks used to play tennis in monasteries.', 'los monjes solían jugar al tenis en los monasterios.') los monnemente juega al tenis en un jal de pascontitas .\n",
            "('in japan, all children go to school.', 'en japón, todos los niños van a la escuela.') en japón , todos los niños al colegio .\n",
            "(\"i'm not willing to cook dinner for twenty people.\", 'no estoy dispuesto a cocinar cena para veinte personas.') no estoy dispuesto a cocinar para la gente a los veinte personas .\n",
            "('did you do it by yourself?', '¿lo hiciste tú solo?') ¿ lo hiciste tú solo ?\n",
            "(\"i don't have time now.\", 'ahora no tengo tiempo.') ahora no tengo tiempo .\n",
            "('people from madrid are weird.', 'la gente de madrid es extraña.') las personas se enormpete de sapro de saímpeados .\n",
            "(\"i've been studying french for a long time, but i'm not fluent.\", 'llevo estudiando francés mucho tiempo, pero no lo domino.') he estado estudiando francés por mucho tiempo , pero no estoy fluidamente .\n",
            "('no one opposed the choice.', 'nadie se opuso a la elección.') nadie se oportaba la elección .\n",
            "(\"here's some water.\", 'aquí hay un poco de agua.') aquí es algo de agua .\n",
            "('you are a good student.', 'eres un buen estudiante.') eres un buen estudiante .\n",
            "('the doll was surprisingly lifelike.', 'la muñeca era sorprendentemente realista.') la muñeca estaba arpectó vidamente .\n",
            "('tom is always serious.', 'tom siempre está serio.') tom siempre es serio .\n",
            "('i need a ride.', 'yo necesito que me lleven.') necesito un achite .\n",
            "('happy thanksgiving!', '¡feliz día de acción de gracias!') ¡ feliz ya es guarde !\n",
            "(\"we'll do as tom wishes.\", 'actuaremos como desea tom.') haremos lo que querrían .\n",
            "('tom refuses to do anything for mary.', 'tom se rehúsa a hacer cualquier cosa por mary.') tom se rehuperar a mary por mary .\n",
            "ROUGE-1 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.6600437760353088>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.636359691619873>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.6415879130363464>}\n",
            "ROUGE-2 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.47827965021133423>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.4601692259311676>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.463918000459671>}\n"
          ]
        }
      ],
      "source": [
        "rouge_1 = keras_hub.metrics.RougeN(order=1)\n",
        "rouge_2 = keras_hub.metrics.RougeN(order=2)\n",
        "\n",
        "for test_pair in test_pairs[:100]:\n",
        "    input_sentence = test_pair[0]\n",
        "    reference_sentence = test_pair[1]\n",
        "\n",
        "    translated_sentence = decode_sequences([input_sentence])\n",
        "#    translated_sentence = translated_sentence.numpy()[0].decode(\"utf-8\")\n",
        "    translated_sentence = (\n",
        "        translated_sentence.replace(\"[PAD]\", \"\")\n",
        "        .replace(\"[START]\", \"\")\n",
        "        .replace(\"[END]\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "\n",
        "    print(test_pair, translated_sentence)\n",
        "\n",
        "    rouge_1(reference_sentence, translated_sentence)\n",
        "    rouge_2(reference_sentence, translated_sentence)\n",
        "\n",
        "print(\"ROUGE-1 Score: \", rouge_1.result())\n",
        "print(\"ROUGE-2 Score: \", rouge_2.result())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-wlmGRXnUjM"
      },
      "source": [
        "After 10 epochs, the scores are as follows:\n",
        "\n",
        "|               | **ROUGE-1** | **ROUGE-2** |\n",
        "|:-------------:|:-----------:|:-----------:|\n",
        "| **Precision** |    0.568    |    0.374    |\n",
        "|   **Recall**  |    0.615    |    0.394    |\n",
        "|  **F1 Score** |    0.579    |    0.381    |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "neural_machine_translation_with_keras_hub",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}