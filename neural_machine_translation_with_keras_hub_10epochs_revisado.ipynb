{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedroconcejero/deep_learning_2024/blob/main/neural_machine_translation_with_keras_hub_10epochs_revisado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZJa0_h7nUiY"
      },
      "source": [
        "# English-to-Spanish translation with KerasHub\n",
        "\n",
        "**Author:** [Abheesht Sharma](https://github.com/abheesht17/)<br>\n",
        "**Date created:** 2022/05/26<br>\n",
        "**Last modified:** 2024/04/30<br>\n",
        "**Description:** Use KerasHub to train a sequence-to-sequence Transformer model on the machine translation task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU7E97kInUic"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "KerasHub provides building blocks for NLP (model layers, tokenizers, metrics, etc.) and\n",
        "makes it convenient to construct NLP pipelines.\n",
        "\n",
        "In this example, we'll use KerasHub layers to build an encoder-decoder Transformer\n",
        "model, and train it on the English-to-Spanish machine translation task.\n",
        "\n",
        "This example is based on the\n",
        "[English-to-Spanish NMT\n",
        "example](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/)\n",
        "by [fchollet](https://twitter.com/fchollet). The original example is more low-level\n",
        "and implements layers from scratch, whereas this example uses KerasHub to show\n",
        "some more advanced approaches, such as subword tokenization and using metrics\n",
        "to compute the quality of generated translations.\n",
        "\n",
        "You'll learn how to:\n",
        "\n",
        "- Tokenize text using `keras_hub.tokenizers.WordPieceTokenizer`.\n",
        "- Implement a sequence-to-sequence Transformer model using KerasHub's\n",
        "`keras_hub.layers.TransformerEncoder`, `keras_hub.layers.TransformerDecoder` and\n",
        "`keras_hub.layers.TokenAndPositionEmbedding` layers, and train it.\n",
        "- Use `keras_hub.samplers` to generate translations of unseen input sentences\n",
        " using the top-p decoding strategy!\n",
        "\n",
        "Don't worry if you aren't familiar with KerasHub. This tutorial will start with\n",
        "the basics. Let's dive right in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s-a7oE1nUie"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Before we start implementing the pipeline, let's import all the libraries we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvGONBUZnUig",
        "outputId": "3438225c-b95e-426b-b923-d40bebc0ead5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.1/644.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade rouge-score\n",
        "!pip install -q --upgrade keras-hub\n",
        "!pip install -q --upgrade keras  # Upgrade to Keras 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Ohunu2-nUij"
      },
      "outputs": [],
      "source": [
        "import keras_hub\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "import keras\n",
        "from keras import ops\n",
        "\n",
        "import tensorflow.data as tf_data\n",
        "from tensorflow_text.tools.wordpiece_vocab import (\n",
        "    bert_vocab_from_dataset as bert_vocab,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-egX3SunnUil"
      },
      "source": [
        "Let's also define our parameters/hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "4aATteT8nUio"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10  # This should be at least 10 for convergence\n",
        "MAX_SEQUENCE_LENGTH = 40\n",
        "ENG_VOCAB_SIZE = 15000\n",
        "SPA_VOCAB_SIZE = 15000\n",
        "\n",
        "EMBED_DIM = 256\n",
        "INTERMEDIATE_DIM = 2048\n",
        "NUM_HEADS = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1qhcFf4nUiq"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "We'll be working with an English-to-Spanish translation dataset\n",
        "provided by [Anki](https://www.manythings.org/anki/). Let's download it:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL del archivo en GitHub\n",
        "url = \"https://raw.githubusercontent.com/pedroconcejero/deep_learning_2024/refs/heads/main/spa.txt\"\n",
        "\n",
        "# Descargar el archivo\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()  # Lanza una excepción si ocurre un error\n",
        "\n",
        "# Procesar las líneas directamente\n",
        "lines = response.text.strip().split(\"\\n\")\n",
        "pairs = [line.split(\"\\t\") for line in lines]\n",
        "\n",
        "# Mostrar algunos pares\n",
        "for eng, spa in pairs[:5]:\n",
        "    print(f\"Inglés: {eng} -> Español: {spa}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXPbsSNHudM5",
        "outputId": "b06feb25-06d6-4618-d975-6c5ab3dd18df"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inglés: Go. -> Español: Ve.\n",
            "Inglés: Go. -> Español: Vete.\n",
            "Inglés: Go. -> Español: Vaya.\n",
            "Inglés: Go. -> Español: Váyase.\n",
            "Inglés: Hi. -> Español: Hola.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mbVtH6Hmtfby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UmiYeSbnUiu"
      },
      "source": [
        "## Parsing the data\n",
        "\n",
        "Each line contains an English sentence and its corresponding Spanish sentence.\n",
        "The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n",
        "Before adding the text to a list, we convert it to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "2qYVMmCcnUiw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Now open the file\n",
        "#with open(text_file) as f:\n",
        "#    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    # Check if the line has a tab character\n",
        "    if \"\\t\" in line:\n",
        "        eng, spa = line.split(\"\\t\")\n",
        "        eng = eng.lower()\n",
        "        spa = spa.lower()\n",
        "        text_pairs.append((eng, spa))\n",
        "    else:\n",
        "        # Handle lines without a tab character (e.g., skip them or print a warning)\n",
        "        print(f\"Warning: Skipping line without tab character: {line}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-040GSnnUiy"
      },
      "source": [
        "Here's what our sentence pairs look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP5aW2ownUiz",
        "outputId": "3e05a457-1ddb-46c0-e379-40e5975a1df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('you can do it if you try.', 'lo puedes hacer si lo intentas.')\n",
            "('were they in the library yesterday?', '¿ayer estaban en la biblioteca?')\n",
            "('the pond dried up last summer.', 'la laguna se secó el verano pasado.')\n",
            "('is she sleeping?', '¿está durmiendo?')\n",
            "('whatever do you mean?', '¿qué quieres decir?')\n"
          ]
        }
      ],
      "source": [
        "# Check if text_pairs is empty and provide a message\n",
        "if not text_pairs:\n",
        "    print(\"Error: 'text_pairs' list is empty. Check the input file or data processing.\")\n",
        "else:\n",
        "    for _ in range(5):\n",
        "        print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP64A3cdnUi1"
      },
      "source": [
        "Now, let's split the sentence pairs into a training set, a validation set,\n",
        "and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idCsnIaZnUi3",
        "outputId": "4c8628ce-d46d-4e56-b943-7a542b77be6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964 total pairs\n",
            "83276 training pairs\n",
            "17844 validation pairs\n",
            "17844 test pairs\n"
          ]
        }
      ],
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63lgx7eLnUi5"
      },
      "source": [
        "## Tokenizing the data\n",
        "\n",
        "We'll define two tokenizers - one for the source language (English), and the other\n",
        "for the target language (Spanish). We'll be using\n",
        "`keras_hub.tokenizers.WordPieceTokenizer` to tokenize the text.\n",
        "`keras_hub.tokenizers.WordPieceTokenizer` takes a WordPiece vocabulary\n",
        "and has functions for tokenizing the text, and detokenizing sequences of tokens.\n",
        "\n",
        "Before we define the two tokenizers, we first need to train them on the dataset\n",
        "we have. The WordPiece tokenization algorithm is a subword tokenization algorithm;\n",
        "training it on a corpus gives us a vocabulary of subwords. A subword tokenizer\n",
        "is a compromise between word tokenizers (word tokenizers need very large\n",
        "vocabularies for good coverage of input words), and character tokenizers\n",
        "(characters don't really encode meaning like words do). Luckily, KerasHub\n",
        "makes it very simple to train WordPiece on a corpus with the\n",
        "`keras_hub.tokenizers.compute_word_piece_vocabulary` utility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "S5MOiTscnUi6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
        "    word_piece_ds = tf_data.Dataset.from_tensor_slices(text_samples)\n",
        "    vocab = keras_hub.tokenizers.compute_word_piece_vocabulary(\n",
        "        word_piece_ds.batch(1000).prefetch(2),\n",
        "        vocabulary_size=vocab_size,\n",
        "        reserved_tokens=reserved_tokens,\n",
        "    )\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G18MpxTknUi7"
      },
      "source": [
        "Every vocabulary has a few special, reserved tokens. We have four such tokens:\n",
        "\n",
        "- `\"[PAD]\"` - Padding token. Padding tokens are appended to the input sequence\n",
        "length when the input sequence length is shorter than the maximum sequence length.\n",
        "- `\"[UNK]\"` - Unknown token.\n",
        "- `\"[START]\"` - Token that marks the start of the input sequence.\n",
        "- `\"[END]\"` - Token that marks the end of the input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "UDO_VZgOnUi8"
      },
      "outputs": [],
      "source": [
        "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
        "eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)\n",
        "\n",
        "spa_samples = [text_pair[1] for text_pair in train_pairs]\n",
        "spa_vocab = train_word_piece(spa_samples, SPA_VOCAB_SIZE, reserved_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeKs-3scnUi-"
      },
      "source": [
        "Let's see some tokens!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Eu7fYx8nUi_",
        "outputId": "070b4c34-f204-4c02-f062-a63e09e7f673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Tokens:  ['know', 'him', 'there', 'go', 'they', 'her', 'has', 'will', 're', 'time']\n",
            "Spanish Tokens:  ['qué', 'ella', 'te', 'mary', 'para', 'las', 'más', 'al', 'yo', 'tu']\n"
          ]
        }
      ],
      "source": [
        "print(\"English Tokens: \", eng_vocab[100:110])\n",
        "print(\"Spanish Tokens: \", spa_vocab[100:110])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq5Jk011nUjA"
      },
      "source": [
        "Now, let's define the tokenizers. We will configure the tokenizers with the\n",
        "the vocabularies trained above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "V7TzbKRJnUjB"
      },
      "outputs": [],
      "source": [
        "eng_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=eng_vocab, lowercase=False\n",
        ")\n",
        "spa_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=spa_vocab, lowercase=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBL4V2E5nUjB"
      },
      "source": [
        "Let's try and tokenize a sample from our dataset! To verify whether the text has\n",
        "been tokenized correctly, we can also detokenize the list of tokens back to the\n",
        "original text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQdlKfy5nUjC",
        "outputId": "5b186a45-4c82-4f04-874b-5f2c656fdc57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sentence:  he has a prejudice against foreigners.\n",
            "Tokens:  tf.Tensor([  70  106   26   41 1949 3623 2522 1188  538 3017   12], shape=(11,), dtype=int32)\n",
            "Recovered text after detokenizing:  he has a prejudice against foreigners .\n",
            "\n",
            "Spanish sentence:  tiene prejuicios contra los extranjeros.\n",
            "Tokens:  tf.Tensor([ 114   44 1261 4958 1287 4207  602   95 2248   15], shape=(10,), dtype=int32)\n",
            "Recovered text after detokenizing:  tiene prejuicios contra los extranjeros .\n"
          ]
        }
      ],
      "source": [
        "eng_input_ex = text_pairs[0][0]\n",
        "eng_tokens_ex = eng_tokenizer.tokenize(eng_input_ex)\n",
        "print(\"English sentence: \", eng_input_ex)\n",
        "print(\"Tokens: \", eng_tokens_ex)\n",
        "print(\n",
        "    \"Recovered text after detokenizing: \",\n",
        "    eng_tokenizer.detokenize(eng_tokens_ex),\n",
        ")\n",
        "\n",
        "print()\n",
        "\n",
        "spa_input_ex = text_pairs[0][1]\n",
        "spa_tokens_ex = spa_tokenizer.tokenize(spa_input_ex)\n",
        "print(\"Spanish sentence: \", spa_input_ex)\n",
        "print(\"Tokens: \", spa_tokens_ex)\n",
        "print(\n",
        "    \"Recovered text after detokenizing: \",\n",
        "    spa_tokenizer.detokenize(spa_tokens_ex),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo4Zh4JLnUjD"
      },
      "source": [
        "## Format datasets\n",
        "\n",
        "Next, we'll format our datasets.\n",
        "\n",
        "At each training step, the model will seek to predict target words N+1 (and beyond)\n",
        "using the source sentence and the target words 0 to N.\n",
        "\n",
        "As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n",
        "\n",
        "- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n",
        "`encoder_inputs` is the tokenized source sentence and `decoder_inputs` is the target\n",
        "sentence \"so far\",\n",
        "that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target\n",
        "sentence.\n",
        "- `target` is the target sentence offset by one step:\n",
        "it provides the next words in the target sentence -- what the model will try to predict.\n",
        "\n",
        "We will add special tokens, `\"[START]\"` and `\"[END]\"`, to the input Spanish\n",
        "sentence after tokenizing the text. We will also pad the input to a fixed length.\n",
        "This can be easily done using `keras_hub.layers.StartEndPacker`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "YIzrf77snUjD"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_batch(eng, spa):\n",
        "    batch_size = ops.shape(spa)[0]\n",
        "\n",
        "    eng = eng_tokenizer(eng)\n",
        "    spa = spa_tokenizer(spa)\n",
        "\n",
        "    # Pad `eng` to `MAX_SEQUENCE_LENGTH`.\n",
        "    eng_start_end_packer = keras_hub.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "        pad_value=eng_tokenizer.token_to_id(\"[PAD]\"),\n",
        "    )\n",
        "    eng = eng_start_end_packer(eng)\n",
        "\n",
        "    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `spa` and pad it as well.\n",
        "    spa_start_end_packer = keras_hub.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
        "        start_value=spa_tokenizer.token_to_id(\"[START]\"),\n",
        "        end_value=spa_tokenizer.token_to_id(\"[END]\"),\n",
        "        pad_value=spa_tokenizer.token_to_id(\"[PAD]\"),\n",
        "    )\n",
        "    spa = spa_start_end_packer(spa)\n",
        "\n",
        "    return (\n",
        "        {\n",
        "            \"encoder_inputs\": eng,\n",
        "            \"decoder_inputs\": spa[:, :-1],\n",
        "        },\n",
        "        spa[:, 1:],\n",
        "    )\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXCC_Xb-nUjE"
      },
      "source": [
        "Let's take a quick look at the sequence shapes\n",
        "(we have batches of 64 pairs, and all sequences are 40 steps long):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3Nt58RynUjF",
        "outputId": "4cb8b201-ba4f-4beb-c9ff-857fc6dc5014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 40)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 40)\n",
            "targets.shape: (64, 40)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7OuW6OznUjG"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Now, let's move on to the exciting part - defining our model!\n",
        "We first need an embedding layer, i.e., a vector for every token in our input sequence.\n",
        "This embedding layer can be initialised randomly. We also need a positional\n",
        "embedding layer which encodes the word order in the sequence. The convention is\n",
        "to add these two embeddings. KerasHub has a `keras_hub.layers.TokenAndPositionEmbedding `\n",
        "layer which does all of the above steps for us.\n",
        "\n",
        "Our sequence-to-sequence Transformer consists of a `keras_hub.layers.TransformerEncoder`\n",
        "layer and a `keras_hub.layers.TransformerDecoder` layer chained together.\n",
        "\n",
        "The source sequence will be passed to `keras_hub.layers.TransformerEncoder`, which\n",
        "will produce a new representation of it. This new representation will then be passed\n",
        "to the `keras_hub.layers.TransformerDecoder`, together with the target sequence\n",
        "so far (target words 0 to N). The `keras_hub.layers.TransformerDecoder` will\n",
        "then seek to predict the next words in the target sequence (N+1 and beyond).\n",
        "\n",
        "A key detail that makes this possible is causal masking.\n",
        "The `keras_hub.layers.TransformerDecoder` sees the entire sequence at once, and\n",
        "thus we must make sure that it only uses information from target tokens 0 to N\n",
        "when predicting token N+1 (otherwise, it could use information from the future,\n",
        "which would result in a model that cannot be used at inference time). Causal masking\n",
        "is enabled by default in `keras_hub.layers.TransformerDecoder`.\n",
        "\n",
        "We also need to mask the padding tokens (`\"[PAD]\"`). For this, we can set the\n",
        "`mask_zero` argument of the `keras_hub.layers.TokenAndPositionEmbedding` layer\n",
        "to True. This will then be propagated to all subsequent layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "gMiuwn4GnUjG"
      },
      "outputs": [],
      "source": [
        "# Encoder\n",
        "encoder_inputs = keras.Input(shape=(None,), name=\"encoder_inputs\")\n",
        "\n",
        "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=ENG_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        ")(encoder_inputs)\n",
        "\n",
        "encoder_outputs = keras_hub.layers.TransformerEncoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(inputs=x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = keras.Input(shape=(None,), name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
        "\n",
        "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=SPA_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        ")(decoder_inputs)\n",
        "\n",
        "x = keras_hub.layers.TransformerDecoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "decoder_outputs = keras.layers.Dense(SPA_VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "decoder = keras.Model(\n",
        "    [\n",
        "        decoder_inputs,\n",
        "        encoded_seq_inputs,\n",
        "    ],\n",
        "    decoder_outputs,\n",
        ")\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    decoder_outputs,\n",
        "    name=\"transformer\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_73Rj0RynUjI"
      },
      "source": [
        "## Training our model\n",
        "\n",
        "We'll use accuracy as a quick way to monitor training progress on the validation data.\n",
        "Note that machine translation typically uses BLEU scores as well as other metrics,\n",
        "rather than accuracy. However, in order to use metrics like ROUGE, BLEU, etc. we\n",
        "will have decode the probabilities and generate the text. Text generation is\n",
        "computationally expensive, and performing this during training is not recommended.\n",
        "\n",
        "Here we only train for 1 epoch, but to get the model to actually converge\n",
        "you should train for at least 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "D8vuJ0VunUjJ",
        "outputId": "065094f5-f8f7-45c4-acc4-91d266adb7cf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"transformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_and_position_embed… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m3,850,240\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbeddi…\u001b[0m │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m1,315,072\u001b[0m │ token_and_position_em… │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional_3 (\u001b[38;5;33mFunctional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15000\u001b[0m)    │      \u001b[38;5;34m9,283,992\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                           │                        │                │ transformer_encoder_1… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_and_position_embed… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,850,240</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbeddi…</span> │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,072</span> │ token_and_position_em… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,283,992</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                           │                        │                │ transformer_encoder_1… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,449,304\u001b[0m (55.12 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,449,304</span> (55.12 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,449,304\u001b[0m (55.12 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,449,304</span> (55.12 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 77ms/step - accuracy: 0.8164 - loss: 1.4898 - val_accuracy: 0.8655 - val_loss: 0.8074\n",
            "Epoch 2/10\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 66ms/step - accuracy: 0.8700 - loss: 0.7848 - val_accuracy: 0.8893 - val_loss: 0.6289\n",
            "Epoch 3/10\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 65ms/step - accuracy: 0.8910 - loss: 0.6250 - val_accuracy: 0.9006 - val_loss: 0.5473\n",
            "Epoch 4/10\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 64ms/step - accuracy: 0.9013 - loss: 0.5477 - val_accuracy: 0.9072 - val_loss: 0.5074\n",
            "Epoch 5/10\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 64ms/step - accuracy: 0.9084 - loss: 0.5000 - val_accuracy: 0.9114 - val_loss: 0.4844\n",
            "Epoch 6/10\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 64ms/step - accuracy: 0.9134 - loss: 0.4679 - val_accuracy: 0.9138 - val_loss: 0.4731\n",
            "Epoch 7/10\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 64ms/step - accuracy: 0.9177 - loss: 0.4412 - val_accuracy: 0.9159 - val_loss: 0.4656\n",
            "Epoch 8/10\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 65ms/step - accuracy: 0.9209 - loss: 0.4201 - val_accuracy: 0.9170 - val_loss: 0.4644\n",
            "Epoch 9/10\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 65ms/step - accuracy: 0.9241 - loss: 0.4008 - val_accuracy: 0.9183 - val_loss: 0.4634\n",
            "Epoch 10/10\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 65ms/step - accuracy: 0.9270 - loss: 0.3838 - val_accuracy: 0.9187 - val_loss: 0.4616\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f08cdbe3760>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YapPMRTTnUjK"
      },
      "source": [
        "## Decoding test sentences (qualitative analysis)\n",
        "\n",
        "Finally, let's demonstrate how to translate brand new English sentences.\n",
        "We simply feed into the model the tokenized English sentence\n",
        "as well as the target token `\"[START]\"`. The model outputs probabilities of the\n",
        "next token. We then we repeatedly generated the next token conditioned on the\n",
        "tokens generated so far, until we hit the token `\"[END]\"`.\n",
        "\n",
        "For decoding, we will use the `keras_hub.samplers` module from\n",
        "KerasHub. Greedy Decoding is a text decoding method which outputs the most\n",
        "likely next token at each time step, i.e., the token with the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6hrXge7nUjK",
        "outputId": "3fd06a8c-e568-41cf-b3d2-fdf77e027599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Example 0 **\n",
            "we'll hide it.\n",
            "nos a ir a la escuela .\n",
            "\n",
            "** Example 1 **\n",
            "oh, by the way, i have something to give you.\n",
            "a la que , tengo la que te lo que te gusta hacer algo .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Error!\n",
        "# A partir de Gemini:\n",
        "# The variable encoder_input_tokens is likely already an EagerTensor,\n",
        "#created by the eng_tokenizer function. Therefore,\n",
        "# calling to_tensor() on it again is causing the error.\n",
        "\n",
        "def decode_sequences(input_sentences):\n",
        "    batch_size = 1\n",
        "\n",
        "    # Tokenize the encoder input.\n",
        "    encoder_input_tokens = ops.convert_to_tensor(eng_tokenizer(input_sentences))\n",
        "    if len(encoder_input_tokens[0]) < MAX_SEQUENCE_LENGTH:\n",
        "        pads = ops.full((1, MAX_SEQUENCE_LENGTH - len(encoder_input_tokens[0])), 0)\n",
        "        # Remove the redundant .to_tensor() call\n",
        "        encoder_input_tokens = ops.concatenate(\n",
        "            [encoder_input_tokens, pads], 1 # Changed from encoder_input_tokens.to_tensor() to encoder_input_tokens\n",
        "        )\n",
        "\n",
        "    # Define a function that outputs the next token's probability given the\n",
        "    # input sequence.\n",
        "    def next(prompt, cache, index):\n",
        "        logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n",
        "        # Ignore hidden states for now; only needed for contrastive search.\n",
        "        hidden_states = None\n",
        "        return logits, hidden_states, cache\n",
        "\n",
        "    # Build a prompt of length 40 with a start token and padding tokens.\n",
        "    length = 40\n",
        "    start = ops.full((batch_size, 1), spa_tokenizer.token_to_id(\"[START]\"))\n",
        "    pad = ops.full((batch_size, length - 1), spa_tokenizer.token_to_id(\"[PAD]\"))\n",
        "    prompt = ops.concatenate((start, pad), axis=-1)\n",
        "\n",
        "    generated_tokens = keras_hub.samplers.GreedySampler()(\n",
        "        next,\n",
        "        prompt,\n",
        "        stop_token_ids=[spa_tokenizer.token_to_id(\"[END]\")],\n",
        "        index=1,  # Start sampling after start token.\n",
        "    )\n",
        "    generated_sentences = spa_tokenizer.detokenize(generated_tokens)\n",
        "\n",
        "    # Check if generated_sentences is a list and extract the first element\n",
        "    if isinstance(generated_sentences, list):\n",
        "        generated_sentences = generated_sentences[0]\n",
        "\n",
        "    return generated_sentences\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for i in range(2):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequences([input_sentence])\n",
        "\n",
        "    # The translated variable now holds a string (or bytes)\n",
        "    # and can be decoded directly.\n",
        "#    translated = translated.decode(\"utf-8\")\n",
        "\n",
        "    translated = (\n",
        "        translated.replace(\"[PAD]\", \"\")\n",
        "        .replace(\"[START]\", \"\")\n",
        "        .replace(\"[END]\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "    print(f\"** Example {i} **\")\n",
        "    print(input_sentence)\n",
        "    print(translated)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09oG33zAnUjL"
      },
      "source": [
        "## Evaluating our model (quantitative analysis)\n",
        "\n",
        "There are many metrics which are used for text generation tasks. Here, to\n",
        "evaluate translations generated by our model, let's compute the ROUGE-1 and\n",
        "ROUGE-2 scores. Essentially, ROUGE-N is a score based on the number of common\n",
        "n-grams between the reference text and the generated text. ROUGE-1 and ROUGE-2\n",
        "use the number of common unigrams and bigrams, respectively.\n",
        "\n",
        "We will calculate the score over 30 test samples (since decoding is an\n",
        "expensive process)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghoBIWm0nUjM",
        "outputId": "8bba17de-40b9-4fd2-c487-3713a7301e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"since my mother was sick, i couldn't go there.\", 'como mi madre estaba enferma, no pude ir allí.') como mi madre estaba enfermo , no podía ir allí .\n",
            "('he has a wife and two young children to provide for.', 'él tiene una esposa y dos niños pequeños que mantener.') él tiene una esposa y dos hijos a la joven para afrenta .\n",
            "('first of all, you should talk it over with your parents.', 'antes que nada, deberías conversarlo con tus padres.') a primera vez , deberías hablar con tus padres con tus padres .\n",
            "('tom is just plain lazy.', 'tomás es simplemente un vago atómico.') tomás es sólo perrito .\n",
            "('i fear for the future of mankind.', 'temo por el futuro del género humano.') me temo para el futuro de la indifincina .\n",
            "('what kind of wine do you have?', '¿qué tipo de vino tienes?') ¿ qué clase de vino ?\n",
            "('you made it.', 'lo has hecho.') lo has hecho .\n",
            "('what is she worried about?', '¿qué la preocupa?') ¿ de qué ella está preocupado ?\n",
            "('i am beginning to remember it.', 'estoy empezando a acordarme.') estoy empezando a recordarlo .\n",
            "('did you wash your hands?', '¿te has lavado las manos?') ¿ te has redo las manos ?\n",
            "('somebody, open this door, please.', 'que alguien abra esta puerta, por favor.') alguien , abre esta puerta , por favor .\n",
            "('tom went ballistic.', 'tom se puso loco.') tom fue bolacista .\n",
            "('i can easily touch my toes.', 'puedo tocarme los dedos de los pies con facilidad.') puedo tocar con mis contactos .\n",
            "(\"that's our secret.\", 'ese es nuestro secreto.') eso es nuestro secreto .\n",
            "('your answer is still incorrect.', 'tu respuesta aún es incorrecta.') tu respuesta es todavía increcia .\n",
            "(\"i wasn't following the conversation.\", 'no estaba siguiendo la conversación.') no estaba siguiendo la conversación .\n",
            "('he was fired for stealing.', 'lo despidieron por robar.') él fue despedido por haber robado .\n",
            "('she has a soft and clear voice.', 'ella tiene una voz suave y clara.') ella tiene una voz ext .\n",
            "('i realized that cats can see in the dark.', 'noté que los gatos pueden ver en la oscuridad.') me di cuenta que los gatos pueden ver en la oscuridad .\n",
            "('tom spent all day looking around antique shops.', 'tom se pasó todo el día mirando tiendas de antigüedades.') tom pasó todo el día enteros por las tienda .\n",
            "('how tall is he?', '¿cómo de alto es él?') ¿ qué alto es él ?\n",
            "('i have a very important meeting.', 'tengo una reunión muy importante.') tengo una reunión muy importante .\n",
            "(\"i'm allergic to seafood.\", 'soy alérgica a los mariscos.') soy alérgico a los mariscos .\n",
            "(\"ice cream was tom's favorite food.\", 'el helado era la comida favorita de tomás.') crema la crema de la comida de tom de la comida de comida de tom .\n",
            "('tom gave his seat to an elderly lady.', 'tom le cedió su asiento a una anciana.') tom le dio su sitio a la dama a la dama .\n",
            "('the main islands of japan are hokkaido, shikoku, honshu and kyushu.', 'las islas principales de japón son hokkaido, shikoku, honshu y kyushu.') la islamichíe de japón es de japón , es la chululu .\n",
            "('the man answers the description.', 'el hombre responde a la descripción.') las respuestas son la descriplación .\n",
            "(\"he's not my father.\", 'él no es mi padre.') él no es mi padre .\n",
            "('i want a lot more.', 'quiero mucho más.') quiero mucho más .\n",
            "('did you want to see me?', '¿querías verme?') ¿ quería verme ?\n",
            "('i feel much better now.', 'ahora estoy mucho mejor.') tengo mucho mejor ahora .\n",
            "('we were made to stay in a small room.', 'nos hicieron quedarnos en una habitación pequeña.') me permanecer en una pequeña habitación pequeña habitación pequeña .\n",
            "('something important has come up.', 'algo importante surgió.') algo importante ha levantado .\n",
            "(\"he's my half-brother.\", 'es mi hermanastro.') él es mi hermano de la mitad .\n",
            "(\"the brake didn't work.\", 'no funcionaban los frenos.') el boltaba far no trabajo .\n",
            "('i was just wondering if tom could possibly be in love with mary.', 'me preguntaba si cabría la posibilidad de que tom esté enamorado de mary.') me estaba preguntando si tom podía ser a la posectado con mary .\n",
            "('he told his children not to make so much noise.', 'él dijo a sus niños que no hicieran tanto ruido.') él le dijo a sus hijos tanto ruido para hacer eso .\n",
            "(\"now it's the real thing.\", 'ahora va de veras.') ahora es la cosa .\n",
            "(\"i haven't been to america.\", 'nunca he estado en estados unidos.') no he estado en estados unidos .\n",
            "(\"congress finally approved wilson's proposals.\", 'el congreso finalmente aprobó las propuestas de wilson.') el finalmente , el fin de la propuesta de la propuesta .\n",
            "('what did tom think of your outfit?', '¿qué pensó tom de tu tenida?') ¿ qué se cree a tom de tu conjunto ?\n",
            "('the rain continued all day.', 'siguió lloviendo todo el día.') la lluvia descumpor toda la lluvia .\n",
            "('this wall feels very cold.', 'este muro está muy frío.') esta pared es muy frío .\n",
            "(\"i didn't grow up in boston.\", 'yo no crecí en boston.') no me crecicié en boston .\n",
            "('get out.', 'sal.') sald .\n",
            "('he can read and write.', 'él sabe leer y escribir.') él puede leer y escribir .\n",
            "('that woman blocked my way.', 'esa mujer se interpuso en mi camino.') esa mujer bloqueó mi manera .\n",
            "('mother told him to look after his younger brother.', 'la madre le dijo que cuidara de su hermano menor.') mi madre le dijo que se ve a su hermano menor .\n",
            "('tom makes a difference.', 'tom marca la diferencia.') tomás hace una diferencia .\n",
            "('he has a large truck.', 'él tiene un gran camión.') él tiene una gran camión .\n",
            "(\"no matter how hard i try, i can't swim to that rock.\", 'no importa cuanto lo intente, no puedo nadar hasta esa roca.') no importa como yo no puedo nadar , no puedo nadar .\n",
            "(\"she's older than him.\", 'ella es mayor que él.') ella es mayor que él .\n",
            "('our teacher sometimes speaks quickly.', 'a veces nuestro profesor habla rápido.') nuestro profesor a veces habla rápido .\n",
            "('his prediction has come true.', 'su predicción se ha hecho realidad.') su predicción de predversa .\n",
            "('she was bewitched by his smile.', 'ella fue hechizada por su sonrisa.') ella estaba rechancedo por su sonrisa .\n",
            "('i ran outside.', 'yo corrí afuera.') corrí afuera .\n",
            "('he waited for several seconds and opened the door.', 'él esperó unos segundos y abrió la puerta.') él esperó por varios segundos y abrió la puerta .\n",
            "('he arrived at midnight.', 'él llegó a la medianoche.') llegó a medianoche .\n",
            "('i have one.', 'tengo uno.') tengo una .\n",
            "(\"i don't know what to depend on.\", 'no sé a que atenerme.') no sé qué se le dividente para de lo que se profenda .\n",
            "(\"it's time to put the children to bed.\", 'ya es hora de acostar a los niños.') es hora de que los niños a la cama a la cama .\n",
            "('my father hates the summer heat.', 'mi padre odia el calor del verano.') mi padre de verano odia el verano .\n",
            "('first of all, we have to finish the homework.', 'antes de nada, tenemos que acabar los deberes.') a primeras , tenemos que terminar la tarea .\n",
            "('you always complain about the weather.', 'siempre te quejás del clima.') siempre te quejas del tiempo .\n",
            "('tom leaned against the wall.', 'tom se apoyó contra la pared.') tom se inclinó en contra la pared .\n",
            "('this is for my friend.', 'esto es para mi amiga.') esto es para mi amigo .\n",
            "('why is she so popular?', '¿por qué ella es tan popular?') ¿ por qué ella es tan popular ?\n",
            "('do you want to eat prawns?', '¿quieres comer langostinos?') ¿ quieres comer trad ?\n",
            "('we have nothing to discuss.', 'no tenemos nada que discutir.') no tenemos nada que discutir .\n",
            "('this movie is politically incorrect.', 'esta película es políticamente incorrecta.') esta película es política excrecrecia .\n",
            "('she shook hands with him.', 'ella sacudió manos con él.') ella se le dia la mano .\n",
            "('he bought eggs and milk from a farmer.', 'él le compró huevos y leche a un granjero.') él compró huevos y una granja de una granja .\n",
            "('i wanted to know where the voice was coming from.', 'yo quería saber de dónde venía la voz.') quería saber la voz de dónde estaba .\n",
            "('the rainy season begins in june.', 'la estación de lluvias empieza en junio.') la temporada empieza en junio de junio .\n",
            "('you or i will be chosen.', 'nos elegirán a ti o a mí.') o te rechogho .\n",
            "('she did it carefully.', 'ella lo hizo cuidadosamente.') ella lo hizo con atención .\n",
            "('where can i have my watch repaired?', '¿dónde me pueden arreglar el reloj?') ¿ dónde puedo ver reparado mi reloj ?\n",
            "(\"i don't need tom.\", 'no necesito a tom.') no necesito a tom .\n",
            "('they called the dog rocky.', 'ellos llamaron al perro rocky.') ellos llamaron al perro choca .\n",
            "('all you have to do is wait.', 'todo lo que tienes que hacer es esperar.') todo lo que tienes que esperar es esperar .\n",
            "('a lot of young people went to hawaii this summer.', 'mucha gente joven fue a hawái este verano.') mucha gente joven fue a hawái a hawái este verano .\n",
            "('many people are better off than they used to be.', 'mucha gente está en mejor situación que antes.') muchas personas están mejor que ellos antes de que ellos .\n",
            "(\"i didn't eat dinner last night.\", 'no cené anoche.') no comí la cena ayer por la noche .\n",
            "('tom follows a strict vegan diet.', 'tom sigue una dieta vegana estricta.') tom sigue estricta .\n",
            "(\"i'll take one.\", 'cogeré uno.') yo llevaré uno .\n",
            "('i need a car.', 'necesito un auto.') necesito un auto .\n",
            "('so, are you going to buy that or not?', 'entonces, ¿vas a comprar eso o no?') así que vas , ¿ va a comprar eso no ?\n",
            "(\"don't talk about business while we're dining.\", 'no hables de negocios mientras estamos comiendo.') no hables de negocios mientras estamos de comemos comemos .\n",
            "(\"tom told mary something she didn't want to hear.\", 'tom le dijo a maría algo que ella no quería oír.') tom le dijo a mary que no quería oír .\n",
            "(\"you don't have to stay.\", 'no necesitan quedarse.') no tenés que quedarme .\n",
            "(\"i want to know about tom's new job.\", 'quiero saber acerca del nuevo trabajo de tom.') quiero saber el nuevo trabajo de tom .\n",
            "('tom flipped the switch.', 'tom giró el interruptor.') tom exablagó la floque .\n",
            "(\"you've gotten better.\", 'te has vuelto mejor.') has hecho mejor .\n",
            "(\"he's three years older than she is.\", 'él tiene tres años más que ella.') él es tres años más que ella .\n",
            "('the children told me they loved strawberry ice cream.', 'los niños me dijeron que les encantaba el helado de frutilla.') los niños me dijeron que corulbana .\n",
            "('i heard that tom is going to get married soon.', 'escuché que tomás se va a casar pronto.') oí que tom se va a casar pronto .\n",
            "('i am getting a cold.', 'me estoy resfriando.') me estoy poniendo frío .\n",
            "('she takes care of my children.', 'ella cuida a mis hijos.') ella cuida de mis hijos .\n",
            "('the night was cool.', 'la noche estaba fresca.') la noche estaba fal .\n",
            "('we have a five percent chance of success.', 'tenemos un cinco por ciento de probabilidades de éxito.') tenemos cinco por ciento de por ciento de por ciento de la oportunidad .\n",
            "ROUGE-1 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.6148701310157776>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.6190227270126343>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.6073640584945679>}\n",
            "ROUGE-2 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.4168608784675598>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.4152548313140869>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.4086366295814514>}\n"
          ]
        }
      ],
      "source": [
        "rouge_1 = keras_hub.metrics.RougeN(order=1)\n",
        "rouge_2 = keras_hub.metrics.RougeN(order=2)\n",
        "\n",
        "for test_pair in test_pairs[:100]:\n",
        "    input_sentence = test_pair[0]\n",
        "    reference_sentence = test_pair[1]\n",
        "\n",
        "    translated_sentence = decode_sequences([input_sentence])\n",
        "#    translated_sentence = translated_sentence.numpy()[0].decode(\"utf-8\")\n",
        "    translated_sentence = (\n",
        "        translated_sentence.replace(\"[PAD]\", \"\")\n",
        "        .replace(\"[START]\", \"\")\n",
        "        .replace(\"[END]\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "\n",
        "    print(test_pair, translated_sentence)\n",
        "\n",
        "    rouge_1(reference_sentence, translated_sentence)\n",
        "    rouge_2(reference_sentence, translated_sentence)\n",
        "\n",
        "print(\"ROUGE-1 Score: \", rouge_1.result())\n",
        "print(\"ROUGE-2 Score: \", rouge_2.result())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-wlmGRXnUjM"
      },
      "source": [
        "After 10 epochs, the scores are as follows:\n",
        "\n",
        "|               | **ROUGE-1** | **ROUGE-2** |\n",
        "|:-------------:|:-----------:|:-----------:|\n",
        "| **Precision** |    0.568    |    0.374    |\n",
        "|   **Recall**  |    0.615    |    0.394    |\n",
        "|  **F1 Score** |    0.579    |    0.381    |"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "neural_machine_translation_with_keras_hub",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}