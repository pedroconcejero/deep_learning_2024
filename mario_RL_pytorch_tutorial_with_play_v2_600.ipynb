{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedroconcejero/deep_learning_2024/blob/main/mario_RL_pytorch_tutorial_with_play_v2_600.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYbmoc0T9kze"
      },
      "source": [
        "Train a Mario-playing RL Agent\n",
        "==============================\n",
        "\n",
        "Para la asignatura INAR 2024-25, a partir de tutorial original disponible aquí:\n",
        "\n",
        "https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html\n",
        "\n",
        "Hay cambios con respecto al original:\n",
        "- Se modifica la configuración de guardado de checkpoints, del original (absurdo) 5e5 (5 * 10⁵, 500000) a 1e4 (Pedro Concejero)\n",
        "- En esta versión se piden 600 episodios de entrenamiento (Pedro Concejero)\n",
        "- Se ha añadido la programación necesaria para visualizar el agente jugando (Víctor Riddlestone). Por cierto, el gif animado que aparece a continuación *no es de este entrenamiento* de hecho, el entorno de juego implementado es diferente (se puede ver en el vídeo).\n",
        "\n",
        "**Authors:** [Yuansong Feng](https://github.com/YuansongFeng), [Suraj\n",
        "Subramanian](https://github.com/suraj813), [Howard\n",
        "Wang](https://github.com/hw26), [Steven\n",
        "Guo](https://github.com/GuoYuzhang).\n",
        "\n",
        "This tutorial walks you through the fundamentals of Deep Reinforcement\n",
        "Learning. At the end, you will implement an AI-powered Mario (using\n",
        "[Double Deep Q-Networks](https://arxiv.org/pdf/1509.06461.pdf)) that can\n",
        "play the game by itself.\n",
        "\n",
        "Although no prior knowledge of RL is necessary for this tutorial, you\n",
        "can familiarize yourself with these RL\n",
        "[concepts](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html),\n",
        "and have this handy\n",
        "[cheatsheet](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N)\n",
        "as your companion. The full code is available\n",
        "[here](https://github.com/yuansongFeng/MadMario/).\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/mario.gif)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Vp0mNhs9vJz",
        "outputId": "5f82aced-67ac-48a5-b2cb-fef11e570c04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym-super-mario-bros==7.4.0\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nes-py>=8.1.4 (from gym-super-mario-bros==7.4.0)\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.26.4)\n",
            "Collecting pyglet<=1.5.21,>=1.4.0 (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0)\n",
            "  Downloading pyglet-1.5.21-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.66.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n",
            "Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp310-cp310-linux_x86_64.whl size=535719 sha256=e99ce5a5d53d5ea6d9265e87e54a7e158d49a12c607be106a676d2d2327949c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/a7/d5/9aa14b15df740a53d41f702e4c795731b6c4da7925deb8476c\n",
            "Successfully built nes-py\n",
            "Installing collected packages: pyglet, nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1 pyglet-1.5.21\n",
            "Collecting tensordict==0.3.0\n",
            "  Downloading tensordict-0.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tensordict==0.3.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensordict==0.3.0) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from tensordict==0.3.0) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->tensordict==0.3.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->tensordict==0.3.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->tensordict==0.3.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->tensordict==0.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->tensordict==0.3.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->tensordict==0.3.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->tensordict==0.3.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->tensordict==0.3.0) (3.0.2)\n",
            "Downloading tensordict-0.3.0-cp310-cp310-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensordict\n",
            "Successfully installed tensordict-0.3.0\n",
            "Collecting torchrl==0.3.0\n",
            "  Downloading torchrl-0.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (31 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchrl==0.3.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchrl==0.3.0) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchrl==0.3.0) (24.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from torchrl==0.3.0) (3.1.0)\n",
            "Requirement already satisfied: tensordict>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from torchrl==0.3.0) (0.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->torchrl==0.3.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->torchrl==0.3.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->torchrl==0.3.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->torchrl==0.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->torchrl==0.3.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->torchrl==0.3.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->torchrl==0.3.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->torchrl==0.3.0) (3.0.2)\n",
            "Downloading torchrl-0.3.0-cp310-cp310-manylinux1_x86_64.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchrl\n",
            "Successfully installed torchrl-0.3.0\n",
            "Requirement already satisfied: nes_py in /usr/local/lib/python3.10/dist-packages (8.2.1)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.10/dist-packages (from nes_py) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from nes_py) (1.26.4)\n",
            "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from nes_py) (1.5.21)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from nes_py) (4.66.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes_py) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes_py) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "# Needed to import libraries in next cell\n",
        "\n",
        "!pip install gym-super-mario-bros==7.4.0\n",
        "!pip install tensordict==0.3.0\n",
        "!pip install torchrl==0.3.0\n",
        "!pip install nes_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8r45qzNLEn-",
        "outputId": "eccfe49d-8645-4683-eaea-06962d028739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RrugZlwg9kzi",
        "outputId": "68b76891-1161-4d4a-c25f-961237850457",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchrl/data/replay_buffers/samplers.py:37: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. If you installed TorchRL from PyPI, please report the bug on TorchRL github. If you installed TorchRL locally and/or in development mode, check that you have all the required compiling packages.\n",
            "  warnings.warn(EXTENSION_WARNING)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os\n",
        "\n",
        "# Gym is an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros\n",
        "\n",
        "from tensordict import TensorDict\n",
        "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfY500wz9kzj"
      },
      "source": [
        "RL Definitions\n",
        "==============\n",
        "\n",
        "**Environment** The world that an agent interacts with and learns from.\n",
        "\n",
        "**Action** $a$ : How the Agent responds to the Environment. The set of\n",
        "all possible Actions is called *action-space*.\n",
        "\n",
        "**State** $s$ : The current characteristic of the Environment. The set\n",
        "of all possible States the Environment can be in is called\n",
        "*state-space*.\n",
        "\n",
        "**Reward** $r$ : Reward is the key feedback from Environment to Agent.\n",
        "It is what drives the Agent to learn and to change its future action. An\n",
        "aggregation of rewards over multiple time steps is called **Return**.\n",
        "\n",
        "**Optimal Action-Value function** $Q^*(s,a)$ : Gives the expected return\n",
        "if you start in state $s$, take an arbitrary action $a$, and then for\n",
        "each future time step take the action that maximizes returns. $Q$ can be\n",
        "said to stand for the \"quality\" of the action in a state. We try to\n",
        "approximate this function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WIBiTyQ9kzl"
      },
      "source": [
        "Environment\n",
        "===========\n",
        "\n",
        "Initialize Environment\n",
        "----------------------\n",
        "\n",
        "In Mario, the environment consists of tubes, mushrooms and other\n",
        "components.\n",
        "\n",
        "When Mario makes an action, the environment responds with the changed\n",
        "(next) state, reward and other info.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVyDq_569kzl",
        "outputId": "ebb4d59c-f892-44f3-99bd-fdd099eb2267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 256, 3),\n",
            " 0.0,\n",
            " False,\n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n",
        "if gym.__version__ < '0.26':\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
        "else:\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
        "\n",
        "# Limit the action-space to\n",
        "#   0. walk right\n",
        "#   1. jump right\n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, trunc, info = env.step(action=0)\n",
        "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xr4Bp0E9kzm"
      },
      "source": [
        "Preprocess Environment\n",
        "======================\n",
        "\n",
        "Environment data is returned to the agent in `next_state`. As you saw\n",
        "above, each state is represented by a `[3, 240, 256]` size array. Often\n",
        "that is more information than our agent needs; for instance, Mario's\n",
        "actions do not depend on the color of the pipes or the sky!\n",
        "\n",
        "We use **Wrappers** to preprocess environment data before sending it to\n",
        "the agent.\n",
        "\n",
        "`GrayScaleObservation` is a common wrapper to transform an RGB image to\n",
        "grayscale; doing so reduces the size of the state representation without\n",
        "losing useful information. Now the size of each state: `[1, 240, 256]`\n",
        "\n",
        "`ResizeObservation` downsamples each observation into a square image.\n",
        "New size: `[1, 84, 84]`\n",
        "\n",
        "`SkipFrame` is a custom wrapper that inherits from `gym.Wrapper` and\n",
        "implements the `step()` function. Because consecutive frames don't vary\n",
        "much, we can skip n-intermediate frames without losing much information.\n",
        "The n-th frame aggregates rewards accumulated over each skipped frame.\n",
        "\n",
        "`FrameStack` is a wrapper that allows us to squash consecutive frames of\n",
        "the environment into a single observation point to feed to our learning\n",
        "model. This way, we can identify if Mario was landing or jumping based\n",
        "on the direction of his movement in the previous several frames.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gSD5F9VL9kzo"
      },
      "outputs": [],
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, trunk, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, trunk, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        # permute [H, W, C] array to [C, H, W] tensor\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = T.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose(\n",
        "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
        "        )\n",
        "        observation = transforms(observation).squeeze(0)\n",
        "        return observation\n",
        "\n",
        "\n",
        "# Apply Wrappers to environment\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "if gym.__version__ < '0.26':\n",
        "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
        "else:\n",
        "    env = FrameStack(env, num_stack=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dkasP829kzr"
      },
      "source": [
        "After applying the above wrappers to the environment, the final wrapped\n",
        "state consists of 4 gray-scaled consecutive frames stacked together, as\n",
        "shown above in the image on the left. Each time Mario makes an action,\n",
        "the environment responds with a state of this structure. The structure\n",
        "is represented by a 3-D array of size `[4, 84, 84]`.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/mario_env.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyNMY7RV9kzt"
      },
      "source": [
        "Agent\n",
        "=====\n",
        "\n",
        "We create a class `Mario` to represent our agent in the game. Mario\n",
        "should be able to:\n",
        "\n",
        "-   **Act** according to the optimal action policy based on the current\n",
        "    state (of the environment).\n",
        "-   **Remember** experiences. Experience = (current state, current\n",
        "    action, reward, next state). Mario *caches* and later *recalls* his\n",
        "    experiences to update his action policy.\n",
        "-   **Learn** a better action policy over time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9PggM2QJ9kzu"
      },
      "outputs": [],
      "source": [
        "class Mario:\n",
        "    def __init__():\n",
        "        pass\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
        "        pass\n",
        "\n",
        "    def cache(self, experience):\n",
        "        \"\"\"Add the experience to memory\"\"\"\n",
        "        pass\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"Sample experiences from memory\"\"\"\n",
        "        pass\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddx26G0m9kzu"
      },
      "source": [
        "In the following sections, we will populate Mario's parameters and\n",
        "define his functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN4C-zXr9kzv"
      },
      "source": [
        "Act\n",
        "===\n",
        "\n",
        "For any given state, an agent can choose to do the most optimal action\n",
        "(**exploit**) or a random action (**explore**).\n",
        "\n",
        "Mario randomly explores with a chance of `self.exploration_rate`; when\n",
        "he chooses to exploit, he relies on `MarioNet` (implemented in `Learn`\n",
        "section) to provide the most optimal action.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pnW9CGfR9kzv"
      },
      "outputs": [],
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
        "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "        self.net = self.net.to(device=self.device)\n",
        "\n",
        "        self.exploration_rate = 1\n",
        "        self.exploration_rate_decay = 0.99999975\n",
        "        self.exploration_rate_min = 0.1\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.save_every = 1e4  # no. of experiences between saving Mario Net # changed from crazy 5e5 in original\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "    Given a state, choose an epsilon-greedy action and update value of step.\n",
        "\n",
        "    Inputs:\n",
        "    state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n",
        "    Outputs:\n",
        "    ``action_idx`` (``int``): An integer representing which action Mario will perform\n",
        "    \"\"\"\n",
        "        # EXPLORE\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "\n",
        "        # EXPLOIT\n",
        "        else:\n",
        "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
        "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
        "            action_values = self.net(state, model=\"online\")\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "        # decrease exploration_rate\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # increment step\n",
        "        self.curr_step += 1\n",
        "        return action_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbHChJSe9kzv"
      },
      "source": [
        "Cache and Recall\n",
        "================\n",
        "\n",
        "These two functions serve as Mario's \"memory\" process.\n",
        "\n",
        "`cache()`: Each time Mario performs an action, he stores the\n",
        "`experience` to his memory. His experience includes the current *state*,\n",
        "*action* performed, *reward* from the action, the *next state*, and\n",
        "whether the game is *done*.\n",
        "\n",
        "`recall()`: Mario randomly samples a batch of experiences from his\n",
        "memory, and uses that to learn the game.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "R__ClNZF9kzw"
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):  # subclassing for continuity\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n",
        "        self.batch_size = 32\n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        \"\"\"\n",
        "        Store the experience to self.memory (replay buffer)\n",
        "\n",
        "        Inputs:\n",
        "        state (``LazyFrame``),\n",
        "        next_state (``LazyFrame``),\n",
        "        action (``int``),\n",
        "        reward (``float``),\n",
        "        done(``bool``))\n",
        "        \"\"\"\n",
        "        def first_if_tuple(x):\n",
        "            return x[0] if isinstance(x, tuple) else x\n",
        "        state = first_if_tuple(state).__array__()\n",
        "        next_state = first_if_tuple(next_state).__array__()\n",
        "\n",
        "        state = torch.tensor(state)\n",
        "        next_state = torch.tensor(next_state)\n",
        "        action = torch.tensor([action])\n",
        "        reward = torch.tensor([reward])\n",
        "        done = torch.tensor([done])\n",
        "\n",
        "        # self.memory.append((state, next_state, action, reward, done,))\n",
        "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"\n",
        "        Retrieve a batch of experiences from memory\n",
        "        \"\"\"\n",
        "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
        "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "271Q9ZXC9kzw"
      },
      "source": [
        "Learn\n",
        "=====\n",
        "\n",
        "Mario uses the [DDQN algorithm](https://arxiv.org/pdf/1509.06461) under\n",
        "the hood. DDQN uses two ConvNets - $Q_{online}$ and $Q_{target}$ - that\n",
        "independently approximate the optimal action-value function.\n",
        "\n",
        "In our implementation, we share feature generator `features` across\n",
        "$Q_{online}$ and $Q_{target}$, but maintain separate FC classifiers for\n",
        "each. $\\theta_{target}$ (the parameters of $Q_{target}$) is frozen to\n",
        "prevent updating by backprop. Instead, it is periodically synced with\n",
        "$\\theta_{online}$ (more on this later).\n",
        "\n",
        "Neural Network\n",
        "--------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7eTK2iVt9kzx"
      },
      "outputs": [],
      "source": [
        "class MarioNet(nn.Module):\n",
        "    \"\"\"mini CNN structure\n",
        "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        c, h, w = input_dim\n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.online = self.__build_cnn(c, output_dim)\n",
        "\n",
        "        self.target = self.__build_cnn(c, output_dim)\n",
        "        self.target.load_state_dict(self.online.state_dict())\n",
        "\n",
        "        # Q_target parameters are frozen.\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "        if model == \"online\":\n",
        "            return self.online(input)\n",
        "        elif model == \"target\":\n",
        "            return self.target(input)\n",
        "\n",
        "    def __build_cnn(self, c, output_dim):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc7aEmkT9kzy"
      },
      "source": [
        "TD Estimate & TD Target\n",
        "=======================\n",
        "\n",
        "Two values are involved in learning:\n",
        "\n",
        "**TD Estimate** - the predicted optimal $Q^*$ for a given state $s$\n",
        "\n",
        "$${TD}_e = Q_{online}^*(s,a)$$\n",
        "\n",
        "**TD Target** - aggregation of current reward and the estimated $Q^*$ in\n",
        "the next state $s'$\n",
        "\n",
        "$$a' = argmax_{a} Q_{online}(s', a)$$\n",
        "\n",
        "$${TD}_t = r + \\gamma Q_{target}^*(s',a')$$\n",
        "\n",
        "Because we don't know what next action $a'$ will be, we use the action\n",
        "$a'$ maximizes $Q_{online}$ in the next state $s'$.\n",
        "\n",
        "Notice we use the\n",
        "[\\@torch.no\\_grad()](https://pytorch.org/docs/stable/generated/torch.no_grad.html#no-grad)\n",
        "decorator on `td_target()` to disable gradient calculations here\n",
        "(because we don't need to backpropagate on $\\theta_{target}$).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tKRKrH4V9kzy"
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.gamma = 0.9\n",
        "\n",
        "    def td_estimate(self, state, action):\n",
        "        current_Q = self.net(state, model=\"online\")[\n",
        "            np.arange(0, self.batch_size), action\n",
        "        ]  # Q_online(s,a)\n",
        "        return current_Q\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        next_state_Q = self.net(next_state, model=\"online\")\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        next_Q = self.net(next_state, model=\"target\")[\n",
        "            np.arange(0, self.batch_size), best_action\n",
        "        ]\n",
        "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3eytadF9kzz"
      },
      "source": [
        "Updating the model\n",
        "==================\n",
        "\n",
        "As Mario samples inputs from his replay buffer, we compute $TD_t$ and\n",
        "$TD_e$ and backpropagate this loss down $Q_{online}$ to update its\n",
        "parameters $\\theta_{online}$ ($\\alpha$ is the learning rate `lr` passed\n",
        "to the `optimizer`)\n",
        "\n",
        "$$\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)$$\n",
        "\n",
        "$\\theta_{target}$ does not update through backpropagation. Instead, we\n",
        "periodically copy $\\theta_{online}$ to $\\theta_{target}$\n",
        "\n",
        "$$\\theta_{target} \\leftarrow \\theta_{online}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BGjWpURp9kz0"
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target):\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4WPVDRP9kz1"
      },
      "source": [
        "Save checkpoint\n",
        "===============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_cP_1xoz9kz1"
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def save(self):\n",
        "        save_path = (\n",
        "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "        )\n",
        "        torch.save(\n",
        "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
        "            save_path,\n",
        "        )\n",
        "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZkw2iqy9kz2"
      },
      "source": [
        "Putting it all together\n",
        "=======================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5y3kgoua9kz2"
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.burnin = 1e4  # min. experiences before training\n",
        "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
        "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
        "\n",
        "    def learn(self):\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync_Q_target()\n",
        "\n",
        "        if self.curr_step % self.save_every == 0:\n",
        "            self.save()\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None\n",
        "\n",
        "        # Sample from memory\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "        # Get TD Estimate\n",
        "        td_est = self.td_estimate(state, action)\n",
        "\n",
        "        # Get TD Target\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "        # Backpropagate loss through Q_online\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "        return (td_est.mean().item(), loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuXuUofD9kz3"
      },
      "source": [
        "Logging\n",
        "=======\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KwESm3zt9kz4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
        "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
        "\n",
        "        # History metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # Moving averages, added for every call to record()\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward, loss, q):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {epsilon} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Loss {mean_ep_loss} - \"\n",
        "            f\"Mean Q Value {mean_ep_q} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
        "            plt.clf()\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
        "            plt.legend()\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99NZPMj99kz5"
      },
      "source": [
        "Let's play!\n",
        "===========\n",
        "\n",
        "In this example we run the training loop for 40 episodes, but for Mario\n",
        "to truly learn the ways of his world, we suggest running the loop for at\n",
        "least 40,000 episodes!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xstGAYoz9kz5",
        "outputId": "3de0849b-cfe7-4ff1-c47c-7ebbc59f67ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA: True\n",
            "\n",
            "Episode 0 - Step 40 - Epsilon 0.9999900000487484 - Mean Reward 231.0 - Mean Length 40.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.848 - Time 2024-12-11T12:32:30\n",
            "Episode 20 - Step 4037 - Epsilon 0.9989912589953204 - Mean Reward 695.286 - Mean Length 192.238 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 58.798 - Time 2024-12-11T12:33:28\n",
            "Episode 40 - Step 8020 - Epsilon 0.9979970084194101 - Mean Reward 653.732 - Mean Length 195.61 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 51.695 - Time 2024-12-11T12:34:20\n",
            "MarioNet saved to checkpoints/2024-12-11T12-32-29/mario_net_1.chkpt at step 10000\n",
            "Episode 60 - Step 13129 - Epsilon 0.9967231302845969 - Mean Reward 662.738 - Mean Length 215.23 - Mean Loss 0.177 - Mean Q Value 0.612 - Time Delta 75.661 - Time 2024-12-11T12:35:36\n",
            "Episode 80 - Step 16268 - Epsilon 0.9959412585373149 - Mean Reward 641.333 - Mean Length 200.84 - Mean Loss 0.256 - Mean Q Value 1.276 - Time Delta 49.482 - Time 2024-12-11T12:36:25\n",
            "MarioNet saved to checkpoints/2024-12-11T12-32-29/mario_net_2.chkpt at step 20000\n",
            "Episode 100 - Step 21919 - Epsilon 0.9945352357615572 - Mean Reward 663.45 - Mean Length 218.79 - Mean Loss 0.307 - Mean Q Value 1.958 - Time Delta 88.649 - Time 2024-12-11T12:37:54\n",
            "Episode 120 - Step 25094 - Epsilon 0.9937461365346081 - Mean Reward 635.57 - Mean Length 210.57 - Mean Loss 0.413 - Mean Q Value 3.174 - Time Delta 50.044 - Time 2024-12-11T12:38:44\n",
            "Episode 140 - Step 29212 - Epsilon 0.992723601199465 - Mean Reward 641.37 - Mean Length 211.92 - Mean Loss 0.51 - Mean Q Value 4.38 - Time Delta 66.097 - Time 2024-12-11T12:39:50\n",
            "MarioNet saved to checkpoints/2024-12-11T12-32-29/mario_net_3.chkpt at step 30000\n",
            "Episode 160 - Step 33336 - Epsilon 0.9917006304706378 - Mean Reward 622.32 - Mean Length 202.07 - Mean Loss 0.521 - Mean Q Value 5.609 - Time Delta 65.159 - Time 2024-12-11T12:40:55\n",
            "Episode 180 - Step 36861 - Epsilon 0.9908270791458004 - Mean Reward 634.24 - Mean Length 205.93 - Mean Loss 0.528 - Mean Q Value 6.717 - Time Delta 57.248 - Time 2024-12-11T12:41:52\n",
            "MarioNet saved to checkpoints/2024-12-11T12-32-29/mario_net_4.chkpt at step 40000\n",
            "Episode 200 - Step 41616 - Epsilon 0.9896499331129522 - Mean Reward 611.31 - Mean Length 196.97 - Mean Loss 0.542 - Mean Q Value 7.725 - Time Delta 83.636 - Time 2024-12-11T12:43:16\n",
            "Episode 220 - Step 45527 - Epsilon 0.9886827756659649 - Mean Reward 623.36 - Mean Length 204.33 - Mean Loss 0.553 - Mean Q Value 8.677 - Time Delta 95.618 - Time 2024-12-11T12:44:52\n",
            "Episode 240 - Step 49249 - Epsilon 0.987763234111193 - Mean Reward 617.87 - Mean Length 200.37 - Mean Loss 0.569 - Mean Q Value 9.668 - Time Delta 151.013 - Time 2024-12-11T12:47:23\n",
            "MarioNet saved to checkpoints/2024-12-11T12-32-29/mario_net_5.chkpt at step 50000\n",
            "Episode 260 - Step 54094 - Epsilon 0.9865675300387134 - Mean Reward 626.46 - Mean Length 207.58 - Mean Loss 0.591 - Mean Q Value 10.617 - Time Delta 257.493 - Time 2024-12-11T12:51:40\n",
            "Episode 280 - Step 59046 - Epsilon 0.9853469150003829 - Mean Reward 637.69 - Mean Length 221.85 - Mean Loss 0.618 - Mean Q Value 11.431 - Time Delta 291.202 - Time 2024-12-11T12:56:31\n",
            "MarioNet saved to checkpoints/2024-12-11T12-32-29/mario_net_6.chkpt at step 60000\n",
            "Episode 300 - Step 63670 - Epsilon 0.9842085119479227 - Mean Reward 653.72 - Mean Length 220.54 - Mean Loss 0.659 - Mean Q Value 12.398 - Time Delta 291.614 - Time 2024-12-11T13:01:23\n",
            "Episode 320 - Step 67998 - Epsilon 0.9831441741154113 - Mean Reward 664.37 - Mean Length 224.71 - Mean Loss 0.695 - Mean Q Value 13.209 - Time Delta 287.387 - Time 2024-12-11T13:06:10\n",
            "MarioNet saved to checkpoints/2024-12-11T12-32-29/mario_net_7.chkpt at step 70000\n",
            "Episode 340 - Step 71989 - Epsilon 0.9821637310928464 - Mean Reward 674.58 - Mean Length 227.4 - Mean Loss 0.738 - Mean Q Value 14.095 - Time Delta 274.324 - Time 2024-12-11T13:10:45\n",
            "Episode 360 - Step 75947 - Epsilon 0.9811923606241689 - Mean Reward 669.11 - Mean Length 218.53 - Mean Loss 0.754 - Mean Q Value 14.781 - Time Delta 280.254 - Time 2024-12-11T13:15:25\n",
            "MarioNet saved to checkpoints/2024-12-11T12-32-29/mario_net_8.chkpt at step 80000\n",
            "Episode 380 - Step 81497 - Epsilon 0.9798319000913078 - Mean Reward 671.4 - Mean Length 224.51 - Mean Loss 0.777 - Mean Q Value 15.47 - Time Delta 399.631 - Time 2024-12-11T13:22:05\n",
            "Episode 400 - Step 85810 - Epsilon 0.9787759655958844 - Mean Reward 663.54 - Mean Length 221.4 - Mean Loss 0.783 - Mean Q Value 15.949 - Time Delta 319.075 - Time 2024-12-11T13:27:24\n",
            "MarioNet saved to checkpoints/2024-12-11T12-32-29/mario_net_9.chkpt at step 90000\n",
            "Episode 420 - Step 91787 - Epsilon 0.977314521579694 - Mean Reward 674.75 - Mean Length 237.89 - Mean Loss 0.779 - Mean Q Value 16.445 - Time Delta 459.893 - Time 2024-12-11T13:35:04\n",
            "Episode 440 - Step 97073 - Epsilon 0.9760238532748275 - Mean Reward 677.57 - Mean Length 250.84 - Mean Loss 0.783 - Mean Q Value 16.88 - Time Delta 440.003 - Time 2024-12-11T13:42:24\n",
            "MarioNet saved to checkpoints/2024-12-11T12-32-29/mario_net_10.chkpt at step 100000\n",
            "Episode 460 - Step 101351 - Epsilon 0.9749805536371219 - Mean Reward 692.26 - Mean Length 254.04 - Mean Loss 0.787 - Mean Q Value 17.245 - Time Delta 363.525 - Time 2024-12-11T13:48:27\n",
            "Episode 480 - Step 105920 - Epsilon 0.9738675177640761 - Mean Reward 695.97 - Mean Length 244.23 - Mean Loss 0.807 - Mean Q Value 17.78 - Time Delta 392.58 - Time 2024-12-11T13:55:00\n",
            "MarioNet saved to checkpoints/2024-12-11T12-32-29/mario_net_11.chkpt at step 110000\n",
            "Episode 500 - Step 110321 - Epsilon 0.9727966091352868 - Mean Reward 692.56 - Mean Length 245.11 - Mean Loss 0.826 - Mean Q Value 18.22 - Time Delta 376.71 - Time 2024-12-11T14:01:16\n",
            "Episode 520 - Step 114059 - Epsilon 0.9718879552250062 - Mean Reward 675.63 - Mean Length 222.72 - Mean Loss 0.847 - Mean Q Value 18.762 - Time Delta 320.667 - Time 2024-12-11T14:06:37\n",
            "Episode 540 - Step 119203 - Epsilon 0.9706389104662025 - Mean Reward 663.97 - Mean Length 221.3 - Mean Loss 0.862 - Mean Q Value 19.208 - Time Delta 438.791 - Time 2024-12-11T14:13:56\n",
            "MarioNet saved to checkpoints/2024-12-11T12-32-29/mario_net_12.chkpt at step 120000\n",
            "Episode 560 - Step 125167 - Epsilon 0.9691927660383026 - Mean Reward 695.6 - Mean Length 238.16 - Mean Loss 0.876 - Mean Q Value 19.681 - Time Delta 509.628 - Time 2024-12-11T14:22:26\n",
            "Episode 580 - Step 129838 - Epsilon 0.9680616516021907 - Mean Reward 694.21 - Mean Length 239.18 - Mean Loss 0.869 - Mean Q Value 19.83 - Time Delta 399.494 - Time 2024-12-11T14:29:05\n",
            "MarioNet saved to checkpoints/2024-12-11T12-32-29/mario_net_13.chkpt at step 130000\n",
            "Episode 599 - Step 134721 - Epsilon 0.9668806112173961 - Mean Reward 695.38 - Mean Length 245.18 - Mean Loss 0.887 - Mean Q Value 20.142 - Time Delta 412.249 - Time 2024-12-11T14:35:57\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")\n",
        "print()\n",
        "\n",
        "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "episodes = 600\n",
        "for e in range(episodes):\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    # Play the game!\n",
        "    while True:\n",
        "\n",
        "        # Run agent on the state\n",
        "        action = mario.act(state)\n",
        "\n",
        "        # Agent performs action\n",
        "        next_state, reward, done, trunc, info = env.step(action)\n",
        "\n",
        "        # Remember\n",
        "        mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        # Learn\n",
        "        q, loss = mario.learn()\n",
        "\n",
        "        # Logging\n",
        "        logger.log_step(reward, loss, q)\n",
        "\n",
        "        # Update state\n",
        "        state = next_state\n",
        "\n",
        "        # Check if end of game\n",
        "        if done or info[\"flag_get\"]:\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "\n",
        "    if (e % 20 == 0) or (e == episodes - 1):\n",
        "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X61M_pDFLEoB"
      },
      "outputs": [],
      "source": [
        "# Adding visualization to see Mario play\n",
        "import gym\n",
        "from gym.wrappers import RecordVideo\n",
        "import os\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Function to display videos inline in the notebook\n",
        "def show_video(video_path):\n",
        "    with open(video_path, \"rb\") as video_file:\n",
        "        video_base64 = base64.b64encode(video_file.read()).decode()\n",
        "        video_tag = f'<video width=\"640\" height=\"480\" controls><source src=\"data:video/mp4;base64,{video_base64}\" type=\"video/mp4\"></video>'\n",
        "        return HTML(video_tag)\n",
        "\n",
        "# Create a monitored environment that saves video to a directory\n",
        "env = gym.make('SuperMarioBros-v0')\n",
        "video_dir = './videos'\n",
        "\n",
        "# Ensure the video directory exists\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "\n",
        "# Wrap the environment to record video\n",
        "env = RecordVideo(env, video_dir)\n",
        "\n",
        "# Reset the environment\n",
        "state = env.reset()\n",
        "done = False\n",
        "\n",
        "# Play the game with random actions\n",
        "while not done:\n",
        "    action = env.action_space.sample()  # Random actions instead of model-based actions\n",
        "    state, reward, done, _ = env.step(action)\n",
        "\n",
        "\n",
        "# Display the recorded video in the notebook\n",
        "video_path = os.path.join(video_dir, [f for f in os.listdir(video_dir) if f.endswith('.mp4')][-1])\n",
        "show_video(video_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}